=============================================== 
(This file isn't currently included in the website;
it is just notes for future additions.)



=============================================== 
A cute idiom for making N foo where N is the length of some list:

    labels =   map  (\\ _ = lbl::make_anonymous_codelabel())  nexts;		# src/lib/compiler/back/low/main/main/translate-nextcode-to-treecode-g.pkg



=============================================== 
Apparently the 'for' loop is not properly documented in 
the full monte or reference docs, where one would expect:

    From: spiralofhope <spiralofhope@lavabit.com> 
    Subject: Re: [Mythryl] string multiplication, scope issue with 'for' 
    To: mythryl@mythryl.org 
    Date: Sat, 28 Jan 2012 01:18:05 -0800 
    Reply-To: mythryl@mythryl.org 

    On Sat, 28 Jan 2012 08:41:37 +0100 
    Michele Bini <michele.bini@gmail.com> wrote: 

    > It's actually documented here: 
    > http://mythryl.org/my-Mythryl_for_Loop.html 

    Hidden in the Mythryl for SML Programmers docs! 

    Thanks. 

  It's actually documented here: http://mythryl.org/my-Mythryl_for_Loop.html 

  (It may look like the same link, but it's actually a different page) 

  Michele 
 


=============================================== 
These appear to be suggestions for stuff to add to 
the Best Practices tutorial section on mythryl.org: 

    From: spiralofhope <reply+i-2929640-55d5849fc43933d2f92e7d9e2d964668cec4502b-113412@reply.github.com> 
    Subject: [mythryl] Best Practices notes/ideas (#3) 
    To: mythryl <cynbe-kw-github.8c5f0c@cynbe.us> 
    Date: Sun, 22 Jan 2012 15:47:37 -0800 

    - http://mythryl.org/pipermail/mythryl/2011-October/000482.html 
    - http://mythryl.org/pipermail/mythryl/2011-October/000488.html 

    > clarify globally, abbreviate locally 

    - http://mythryl.org/pipermail/mythryl/2011-November/000569.html 

    > Incidentally, when you can't figure out what the type of something 
    > is, a useful trick is often to just declare it as Int or String or 
    > whatever, compile, and look at the error message:  "foo is declared 
    > Int but actually Sometype".  So then you know it is Sometype. 

    - http://mythryl.org/pipermail/mythryl/2011-November/000576.html 

    > The best advice I can give on this front is: 
    > ... (etc) 

    - http://mythryl.org/pipermail/mythryl/2011-November/000587.html 

    > Here seeing the   bar (a, zot(b), blarg(c)) 
    > line up-front makes the following function definitions 
    > much easier to follow, whereas the 
    > ... 
    > tends be very hard to read because the purpose of bar, zot and blarg 
    > remains totally mysterious while reading them (especially if the 
    > author is given to fn names like 'f, g, h ...' or 'sn, bn, sbn, ...' :-); 
    > they have to be sort of just stuffed undigested in short-term memory 
    > (where they don't fit) until the 
    > ... 
    > line finally arrives and ties it all together. 

    (etc) 
 


=============================================== 
Need to have a tutorial showing how to do compiles 
in scripts using logic like:

    #!/bin/sh 
    # 
    bin/mythryld  src/lib/core/mythryl-compiler-compiler/mythryl-compiler-compiler-for-this-platform.lib  <<EOF 
        mythryl_compiler_compiler_for_this_platform::make_mythryl_compiler (); 
    EOF 
    # 
    exit 0 


=============================================== 
Need to have a tutorial showing how to use a library 
in a script by doing something like 

    #!/usr/bin/mythryl 
    # 
    make7::make "foo.lib"; 
    { 
        foo::this(); 
        foo::that(); 
    } 



=============================================== 
The programmer --- like the sculptor, the writer, the painter,
the poet --- starts with a blank screen and simply by concentrating 
until beads of blood form on the forehead, brings forth creations 
limited only by the imagination.  For those born to create, there 
is no greater joy.

The programmer --- also like the sculptor, the writer, the painter,
the poet --- must also grind through various tedious technical issues 
to bring these creations to the light of day.

The intent of this chapter is to show you how to waste as little time 
as practical on the tedious stuff, freeing you to spend that much more 
time on the creative side.

(If you like the process itself more than the end result, you may of 
course wish to skip this chapter, the better to spend more hours debugging!)




Keep things at the same conceptual level within a function.
E.g., write

    fun do_stuff ()
        = 
        {    do_first_part  ();
             do_second_part ();
             do_third part  ();
        }

not

    fun do_stuff ()
        = 
        {    do_first_part  ();
             do_second_part ();
             for (i = 0; i < limit; ++i) { 
                 if (foo(i)   this(); 
                 else         that(); 
             } 
        } 



Move busywork down into subroutines when practical 

E.g. write 

     fun do_stuff x 
         = 
         { 
             ... 
             do_work x; 
             ... 
         } 
         where 
             fun do_work x 
                 = 
                 if (x.work_to_do) 
                     ... 
                 fi; 
         end; 

in stead of 

     fun do_stuff x 
         = 
         { 
             ... 
             if x.work_to_do   do_work x;       fi; 
             ... 
         }  
         where 
             fun do_work x 
                 = 
                 ... 
         end; 

          


Use the compiler!

Some people are very proud that their programs always compile first try.

They are idiots!  They are wasting time doing by hand work that the 
compiler can do more quickly and reliably. 

Example: If you need to rename constructor SIZE to BYTESIZE in 
some sumtype, consider renaming it at the point of definition, then 
dooing a compile and letting the compiler find all references to it 
for you.  Since SIZE is a common name, this will usually be much more 
reliable and efficient than attempting a global textual 
search-and-replace.


``Clarify Globally, Abbreviate Locally''

Clarity comes first, brevity second.  Briefer is better only when clarity is not compromised.





Make your own mistakes --- don't repeat mine!

Mistakes are how we learn;  if you aren't making mistakes, you're not learning.
As engineers we find out what works and what doesn't by diving in and trying --- 
the ones things that don't work are called 'mistakes'.

But if you want to do cutting-edge stuff, if you want to impress your friends,
if you want make a difference --- change the world! --- you cannot afford to 
waste your life repeating old mistakes.  You have to quickly and efficiently 
learn about all the old mistakes --- so that you can go on to make entirely new 
and wonderful mistakes of your own!

I started programming in 1972 and I've been programming pretty much seven days a week, 
360 days a year since about 1978.  I've always tried to push my boundaries on every 
program I write.  I've learned by making mistakes --- so I've learned a lot!  There 
are even moments now and then when, intoxicated by hubris, I think that someday I may 
actually understand this programming thang.

Following are some lessons hard-won at the keyboard.  You can spend five minutes 
learning them now by reading this --- or you can go out and spend thirty years learning 
them the hard way, by repeating all my mistakes.  It's your life and your choice!  I 
don't care a whole lot either way;  I figure idiots too stupid to learn from the mistakes 
of others -deserve- to suffer. ;-)


(1) He travels fastest who travels alone.
    ====================================

    The internal bandwidth of the human brains is immense; way upwards of gigabytes 
    per second.  The external bandwidth between human brains is two bits per second. 
    When more than three people are centrally involved in a project, costs of concensus 
    scale as O(N**2), further suppressing progress.  This is why no committee ever 
    won an award for brilliance or elegance. 

    Consequently there are enormous programming speed advantages to be gained by keeping 
    the core of a project in one brain.  This is why about 99% of open source projects 
    have one lead programmer plus one or two helpers handling non-core jobs. 


(2) Complexity is the Great Enemy. 
    ============================= 

    Undergrad computer science might give you the impression that the pivotal issues 
    in programming revolve around learning algorithms or library functions or language 
    features. 

    That impression is WRONG!  Learning the relevant algorithms, libraries and language 
    features is an absolute requirement to doing useful programming, but that sort of 
    stuff is the easy part.  

    The hard part is controlling complexity --- packing a useful amount of functionality 
    into a piece of code so small and simple and modular as to be humanly comprehensible. 

    The universe is an enormously complicated place  --- say, 10**160 particles zooming about. 
    We model and interact with that universe in our software, so the natural tendency 
    of our software is to approach that sort of limiting complexity.  But our human noggins 
    can only contains a few tens of megabytes of information.  Consequently it is a constant 
    battle to find ways to make our software complex enough to be useful yet simple enough 
    to be comprehensible and thus constructable.  This is the real battlefield upon which 
    the software developer fights and bleeds and  --- occasionally  --- emerges victorious. 

    Hugh Holbrook pointed out to me that one can only really understand about 10,000 lines 
    of code at a time;  real-world development proceeds largely by loading 10Klines of code 
    into working memory, doing some development, then dumping them and loading another 10Klines 
    of code.  If your code is not sufficiently modular to make this process practical and 
    productive, it will not be humanly maintainable, and consequently won't work.

    In my experience, 100,000 lines of code is the most that one person can effectively maintain.
    The core of every open source project I've ever looked at is 100,000 lines or less;  except 
    for very small projects, it is usually on the close order of 100,000 lines of code.  Even 
    the Linux kernel consists of a core of about 100,000 lines of code that Linus Torvalds really 
    understands, plus various subsystems and drivers that he largely delegates to others.

    So when you're planning a large project, you don't want to be thinking primarily about 
    performance or elegance or algorithms.  All of those are important, but what you want 
    to primarily be thinking about is:  How do I break this down and lay it out so that it 
    is actually possible to build and understand it?
    

(3) Evolution beats Revolution 
    ==========================

    Undergrad programming is mostly about writing small programs from scratch.

    Real-world programming is mostly about modifying existing large programs.
    There are a number of reasons for this, including:

     o Studies consistently show that about 80% of the programming effort 
       put into a software project come during the maintenance phase.
       Initial development is an almost negligible part of the overall 
       development effort.

     o Attempts to develop large programs in one fell swoop  --- write all 
       the parts, plug them together, turn on the power  --- usually fail 
       miserably.  Large robust programs usually evolve incrementally from 
       small robust programs.  The Linux kernel is a sterling example of 
       a program which started out small enough to fit on a floppy and 
       grew incrementally from there.

     The most spectacular failures in programming history have come from 
     trying to write enormous systems from scratch using the big-bang 
     approach.  I'm thinking of the Denver airport baggage handling system 
     that was ultimately abandoned, the repeated efforts to write revolutionary 
     replacements for the US air traffic control software systems, and a 
     variety of similar internal corporate projects whose very existence is 
     now usually vehemently denied.  ``Never happened.''

     The lesson:

         Get a small system working early and evolve it.




Programming Pragmatics 

Program by transformation.  After each transformation, check your work 
by compiling and then running your regression suite.  After it checks 
out, checkpoint your codebase in your sourcecode revision control system. 
(Mine is just ``make tar''  --- sometimes the simplest tools are the best tools.
Use the tools that work for you.)



A tutorial on cascading sumtypes might be helpful.  A relatively tractable 
example industrial example is:

    src/app/makelib/depend/inter-library-dependency-graph.pkg 



Great programmers don't work harder  --- they work smarter.

Studies regularly document productivity differences of X10 and more between 
programmers working side by side with similar backgrounds and tasks.  These 
productivity differences are not due to the better programmers working ten times 
more hours  --- there aren't enough hours in the day for that.  They are due to the 
better programmers working smarter.

Working smarter means never doing by hand that which the computer can do for you 
more quickly and reliably.

Working smarter means actively searching for ways to automate work:

 o The better you know your software tools, the more tasks you 
   can automate.  Figure out which capabilities of which available 
   tools can be applied to your tasks, and learn to use them 
   effectively.  Emacs tags-query-replace with appropriate regular 
   expressions is Your Friend!  Ditto keyboard macros and one-line 
   perlscripts.  (To improve your game, try playing a little Perl golf!)

 o The more you can regularize a set of tasks, the easier it is to 
   automate some or all of your work on them.  This is one of many 
   good reasons to formulate and follow coding conventions. Pointless 
   variation in code and data are the mortal enemy of automation, and 
   thus of programmers.  Regularize, normalize, conventionalize!

 o Watchmakers, machinists, carpenters and programmers are all blessed 
   in that they can build their own tools as needed.  Do so!

   Machinists often spend more time building jigs  --- special purpose 
   tools  --- than  working directly on the workpiece.  Good programmers 
   likewise often spend more time building new tools than working directly 
   on the codebase.

   Every new tool you build not only speeds up your current taskset, but 
   is also an investment in your future, not only directly as a tool, but 
   as experience and insight gained in building tools.

   Some tools get used once and discarded, some get used occasionally, a 
   few Perl go on to become a whole new paradigm.  They are all useful; 
   Any tool is a win if it saves more time than it cost to build.




Don't Be Clever 

``Clever'' is a dirty word in programming circles.  Clever tricks 
make code hard to undestand and maintain.  Clever tricks turn around 
and bite you on the ass when you least expect it and when you have the 
least time to deal with it.  Don't be clever  --- Keep It Simple.


Per Churchill, short words are best, and old words when short are best of all.

Words in package names frequently get abbreviated to just the leading letter so 
when in doubt favor the word with the least common initial letter.  Initial letter frequencies in English 
are:
\begin{verbatim}
x	0.005%
q	0.043%
z	0.050%
v	0.619%
j	0.631%
k	0.690%
u	1.487%
y	1.620%
r	1.653%
g	1.950%
e	2.000%
n	2.365%
p	2.545%
d	2.670%
l	2.705%
c	3.511%
f	3.779%
m	4.374%
b	4.702%
o	6.264%
i	6.286%
w	6.661%
h	7.232%
s	7.755%
a	11.602%
t	16.671%
\begin{verbatim}
(http://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_the_first_letters_of_a_word_in_the_English_language)
so ``junk'' is better than ``stuff'' and
``guts`` is better than ``pith'' which is better than ``core''
and ``queen'' is better than ``king'', everything else being equal.

Try to be consistent: Use the same word to mean the same thing and 
vice versa  --- this makes searching the codebase easier.


With rare exceptions, each sourcefile should define one api or one package, and the filename should match the api or package name.


Brvty s nt a sbsttt fr clrty.  Briefer is better  --- but only when equally clear.


PACKAGES NOT PREFIXES
=====================

Any time you find yourself creating a family of identifiers 
   foo_a
   foo_b
   foo_c
   ...
you should probably stop and switch to using a subpackage so that they are named 
   foo::a
   foo::b
   foo::c
   ...
The advantages of the latter are two:
   1) The reader is clear that 'foo' is just a prefix.
   2) The client can rename the foo:: prefix as desired via 'package bar=foo;',
      or drop it entirely by doing an 'include package   foo;'.  There is no corresponding 
      freedom with the prefix approach short of exhaustive enumeration of synonyms.

NB: The subpackage doesn't have to be at global scope or in another file;
it is sufficient to nest it within the current package and file.

A particularly useful idiom is, instead of writing 

    Int_Op   = IO_ADD 
             | IO_SUBTRACT 
             ...
             ;

    Float_Op = FO_ADD 
             | FO_SUBTRACT 
             ...
             ;

to write 

    package io {
	Op = ADD 
	   | SUBTRACT 
	   ...
	   ;
    }; 
    Int_Op = io::Op;
        
    package fo { 
	Op = ADD 
	   | SUBTRACT 
	   ... 
	   ; 
    }; 
    Float_Op = fo::Op; 
        
This allows the client programmer to choose between 

    case io 
        io::ADD      => ... ; 
        io::SUBTRACT => ... ; 
        ... 
    esac; 

and 

    {   include package   io; 
	# 
	case io 
	    ADD      => ... ; 
	    SUBTRACT => ... ; 
	    ... 
	esac; 
    } 

depending on whether brevity or clarity are most important. 

Type Functions 
============== 

By writing 

    Pair(X) = (X, X); 

we define a compiletime function from types to types; 
given any type, it returns a different type: 

    Int_Pair   = Pair(Int); 
    Float_Pair = Pair(Float); 

Such type functions do not generate unique new types; 
if we write 

    Int_Pair1  = Pair(Int); 
    Int_Pair2  = Pair(Int); 

then Int_Pair1 and Int_Pair2 are synonyms referring to 
exactly the same type, courtesy of Mythryl's pervasive 
computation of type equality by structure rather than name. 

Consequently type functions are essentially an abbreviation mechanism 
allowing types to be declared more compactly and readably. 


TREAT INCLUDE LIKE DYNAMITE 
=========================== 

Dynamite is great stuff;  it can do months of pickaxe work into minutes. 
It can also kill you in a split second if you slip up just once.  So 
you don't store it in the house and don't use it unless you really need it. 

The Mythryl 'include' statement is linguistic dynamite.  Sometimes 
it is exactly what you want but most of the time it is just a great way 
to make your code hard to read and maintain by obscuring the origins of 
identifiers.  I use 'include' mainly to simulate subclassing of packages 
by including one package in another and then selectively adding or overriding 
components.  Used this way, include is Your Friend, and can do much to reduce 
pointless code duplication.  I almost never 'include' a package just for 
purposes of abbreviation; by and large, I consider this to be an abomination, 
so much so that I've been tempted from time to time to remove 'include' from 
``the language just to prevent this. 


Briefer is better  --- but clarity beats brevity 
================================================ 



Sometimes pattern matching is not the best syntax 
================================================= 


Pattern-matching syntax is concise and convenient  --- except when it is not! 

Consider 

       case foo 
	   ...  
	   ncf::GET_ADDRESS_OF_FIELD_I { record, ... } =>   bar record; 
	   ...  
       esac; 

vs 

       case foo 
	   ...  
	   ncf::GET_ADDRESS_OF_FIELD_I r =>   bar r.record; 
	   ...  
       esac; 



Assignment is still just a function 
=================================== 

It is worth emphasizing that ':=' is not a reserved word, and that 
the assignment function it names can be mapped and renamed like 
any other function: 

    al:/mythryl7/mythryl7.110.58/mythryl7.110.58> my 

    Mythryl 110.58.3.0.2 built Wed Mar  2 21:19:40 2011 
    Do   help();   for help.

    eval:  r1 = REF 1; 
    eval:  r2 = REF 2; 
    eval:  apply (:=) [ (r1,10), (r2,11) ]; 

    eval:  printf "%d, %d\n" *r1 *r2; 
    10, 11 

    eval:  set_refcell = (:=); 
    eval:  apply set_refcell [ (r1,20), (r2,21) ]; 
    eval:  printf "%d, %d\n" *r1 *r2; 
    20, 21 

    eval:  nonfix my := ; 
    eval:  := = 12; 
    eval:  := ; 
    12 






Equality Types 
============== 

Code like 

    if (a == b) ... 

is ubiquitous in Mythryl programming. 

Which types may be compared using '=='? 

Types which may be so compared are called ``equality types'': 

Simple equality types are: 

  o Void:    () 
  o Bool:    TRUE, FALSE 
  o Int:     1 2 3 ... 
  o Char:    'a' 'b' 'c' ... 
  o String:  "a" "b" "c" ... 

Additional equality types may be built up by compounding: 

  o Null_Or( Foo )                    is an equality type if Foo is an equality type.  
  o List(    Foo )                    is an equality type if Foo is an equality type.  
  o Vector(  Foo )                    is an equality type if Foo is an equality type. 

  o (Foo, Bar, ... )                  is an equality type if Foo, Bar ... are equality types. 
  o { foo: Foo, bar: Bar, ... }       is an equality type if Foo, Bar ... are equality types. 
  o Ug = FOO Foo | BAR Bar | ...      is an equality type if Foo, Bar ... are equality types. 

Refcells are an important special case.  Every refcell is equal to 
itself and nothing else. The contents of the cell are irrelevant. 
Consequently: 

  o Ref( Foo )                        is an equality type for *any* type Foo. 




Functions, for example, are not equality types. 

Strong sealing can hide the fact that a type is an equality type. 
For example in 

    package foo: api { Foo; } { 
        Foo = Int; 
    }; 

foo::Foo is not an equality type because we have hidden its equality 
attributes.  If we want to reveal to code clients that a type is an 
equality type (without revealing anything else) we write instead 

    package foo: api { eqtype Foo; } { 
        Foo = Int; 
    }; 

Often, of course, we still wish to compare values which do not belong to 
equality types.  In such cases we simply write a boolean-valued 
comparison function, conventionally named same_foo: 

    Funky = { name: String, 
              body: Int -> Int; 
            }; 

    fun same_funky (a, b) 
        = 
        a.name == b.name; 

Here we have side-stepped the fact that functions are not equality 
types  --- and thus neither are our records  --- by comparing only the 
name fields in the record.  Now, instead of writing the (illegal) 

    if (a == b) ... 

we can write 

    if (same_funky (a, b)) ... 

If you like the infix format, you may of course do 

    infix  my 50 === ; 

    === = same_funky; 

somewhere at the top of the file and then write  

    if (a === b) ... 

Usually, however, 'same_funky' is a more readable notation for comparison 
of values not belonging to an equality type. 


From: cynbe@mythryl.org 
To: mythryl@mythryl.org 
Subject: Try 2: Equality type variables and types 
--text follows this line-- 

I spent some more time re-reading the SML Definition and Commentary 
on equality types and such, and I understand the issues and answers 
better now.  (Persistence pays off! :-)  So here's a second try at 
explaining Mythryl equality type variables and types in more or less 
plain English: 

First, a recap on type variables: 

Mythryl lets us write something like 

    Pair_Of_Ints = (Int,Int); 
or 
    Tree_of_Ints = LEAF 
                 | NODE { key:      Int, 
                          leftkid:  Tree_Of_Ints, 
                          rightkid: Tree_Of_Ints 
                        }; 
of 
    fun make_pair_of_ints  (a: Int) =   (a,a); 

and then if we like 

    Pair_Of_Strings = (String,String); 
or 
    Tree_of_Strings = LEAF 
                    | NODE { key:      String, 
                             leftkid:  Tree_Of_Strings, 
                             rightkid: Tree_Of_Strings 
                           }; 
or 
    fun make_pair_of_strings  (a: String) =   (a,a); 


Doing that for type after type can get old, so Mythryl also allows 
us to write such definitions more abstractly using type variables 
in place of concrete types: 

    Pair(X) = (X,X); 

    Tree(X) = LEAF 
            | NODE { key:      X, 
                     leftkid:  Tree(X), 
                     rightkid: Tree(X) 
                   }; 

    fun make_pair  (a: X) =   (a,a); 

Here  make_pair  is a 'typeagnostic' function which does not actually 
care about the types of its 'a' and 'b' arguments, so long as they 
are the same: 

    eval:  pair_of_ints    =  make_pair(1);  
    (1, 1) 

    eval:  pair_of_strings =  make_pair("1"); 
    ("1", "1") 

Pair(X) and Tree(X), by contrast, are compile-time type functions; 
give them a type and they'll return another type: 

    Pair_Of_Ints    =  Pair(Int); 
    Pair_Of_Strings =  Pair(String); 

    Tree_Of_Ints    =  Tree(Int); 
    Tree_Of_Strings =  Tree(String); 

A critical difference here is that Tree() is "generative" 
but Pair() is not.  In practice this means that if we write 

    Intpair1 = Pair(Int); 
    Intpair2 = Pair(Int); 

    Inttree1 = Tree(Int); 
    Inttree2 = Tree(Int); 

then Intpair1 and Intpair2 are synonymous  --- different names 
for the same type  --- but Inttree1 and Inttree2 are different 
types, and values of those types are not interchangable. (The 
difference to key on is that Pair() defines no constructors, 
whereas Tree defines two  --- LEAF and NODE.  Any type declaration 
defining a constructor is generative.) 

So far, so familiar. 

But if we try writing 

    fun eq  (a: X,  b: X) =   (a == b); 

we find that the compiler balks. 

The problem is that if we allowed this function as written we would be 
committed to implementing equality on all types, whereas in fact a 
core Mythryl design decision is not to support equality testing on 
values like functions and closures where no practically tractable and 
mathematically sensible definition of equality is possible.  (E.g., 
the Halting Problem will bite us.) 

To deal with situations like this Mythryl supports "equality type 
variables" which are restricted to taking on types which can sensibly 
be compared for equality  --- "equality types".  We distinguish such 
type variables by a leading underscore: 

    A  B  C ...  X  Y  Z               # Vanilla  type variables. 
   _A _B _C ... _X _Y _Z               # Equality type variables.  

    A_descriptive_typevar        # Long vanilla  type variable:  [A-Z]_[a-z0-9_']+ 
   _A_descriptive_typevar        # Long equality type variable: _[A-Z]_[a-z0-9_']+ 



Using equality type variables we can write a version of 
the above 'eq' function which compiles and runs: 

    eval:  fun eq  (a: _X,  b: _X) =   (a == b);  

    eval:  eq (1,2);  
    FALSE 

    eval:  eq (1,1); 
    TRUE 

    eval:  eq ("1","1"); 
    TRUE 


We may similarly write 

    Pair(_X) = (_X,_X); 

    Tree(_X) = LEAF 
             | NODE { key:      _X, 
                      leftkid:  Tree(_X), 
                      rightkid: Tree(_X) 
                    }; 

    fun make_pair  (a: _X) =   (a,a); 

Now any types produced by Pair() or Tree() will be equality types, 
and any values of those types can be compared for equality using '==', 
and similarly any values returned by make_pair. 


So, just what types are equality types? 

Base equality types are: 

        Type     Sample values 
        =======  ==================  

        _X                            # Or any other equality type-variable. 
        Void     ()   
        Bool     TRUE FALSE           # This is really a special case of sumtypes, treated below. 
        Int      1 2 3 ...            # Or Unt or Int1 or Integer or any other stock integer type. 
        Char     'a' 'b' 'c' ...   
        String   "a" "b" "c" ...   
 
Refcells are an important special case:  Every refcell is equal to 
itself and unequal to every other refcell.  (In essence, refcells 
are compared by address, ignoring their contents.)  The actual 
value held by the refcell plays no part, and consequently the type 
of that value does not matter. 

Our Rw_Vector type is exactly the same story  --- the compiler 
in essence views a refcell as being a length-one Rw_Vector. 

This gives us two additional base cases: 

        Type              Sample values 
        =======           ==================  

        Ref(Foo)          REF(1) REF("1") ...        # For absolutely any type Foo. 
        Rw_Vector(Foo)    #[1,2]  #["1","2"] ...     # For absolutely any type Foo. 

Additional equality types may be built up by compounding:  
 
        Null_Or( Foo )                    is an equality type if Foo is an equality type.   
        List(    Foo )                    is an equality type if Foo is an equality type.   
        Vector(  Foo )                    is an equality type if Foo is an equality type.  
 
        (Foo, Bar, ... )                  is an equality type if Foo, Bar ... are equality types.  
        { foo: Foo, bar: Bar, ... }       is an equality type if Foo, Bar ... are equality types.  
        Ug = FOO Foo | BAR Bar | ...      is an equality type if Foo, Bar ... are equality types.  
     
(NB: Null_Or() and List() are both in fact special-case sumtype 
definitions  --- we list them separately in the interests of putting 
tutorial clarity before mathematical minimality.) 

The interesting corner cases arise from sumtype definitions. 

First, sumtype definitions need not reference any subtypes, 
so our neat bottom-up compounding view of the world breaks 
down in that case: 

      o Color = RED | GREEN | BLUE; 

We include Color as an "equality type" anyhow, because that 
is unproblematic and we'd like to support equality testing 
via '==' on as many types as reasonably possible. 

Also, sumtype definitions can be recursive: 

      o Tree = LEAF | NODE (Tree,Tree); 

Simple cases like this are obviously also unproblematic, so 
we make them "equality types" also. 

Most generally, however, sumtype definitions may refer to 
each other and other types in arbitrarily tangled mutually 
recursive fashion: 

         Foo = FOO_BAR(Bar) | FOO_ZOT(Zot) | FOO_INT(Int) 
         also 
         Bar = BAR_FOO(Foo) | BAR_ZOT(Zot) | BAR_STRING(String) 
         also 
         Zot = ZOT_FOO(Foo) | ZOT_BAR(Bar) | ZOT_ALL(Foo,Bar,Zot) 
         ; 

The mutual recursion defeats our simple bottom-up idea of 
defining equality types to be a few base cases plus 
nested compounds of those cases because we cannot decide 
whether any of Foo, Bar or Zot are equality types in 
isolation; all of them must be considered at once, because 
our vision of a nice clean typetree has turned into a nasty 
tangled typegraph full of loops. 

The basic intuitive answer in such cases is that Mythryl makes 
Foo, Bar and Zot equality types whenever reasonably possible. 
This is in practice usually obvious at a glance, and if in doubt 
you can just run the example in question through the compiler. 

If you want the full mathematical treatment, by all means 
consult the actual Definition. :-) 

It turns out that there is always a unique solution 
producing the maximal number of equality types, so the 
language behavior here is not implementation-dependent, but 
rather fully specified by the Definition. 




We need a tutorial on the scope of type variables.  It might 
help if I figured those scopes out before I wrote it. :-) 



It is worth noting that the argument to a generic package can 
have extra components  --- say, abbreviation names for packages. 


Should mine the mailing list essays for additional website 
material, i.e. the Mythryl Linkage Model essagy. 

Hue White comments: 

> Cynbe, 
> Thinking about this a bit more, I think this subject is definitely worth a place in the tutorial as compilation environments are 
> rarely if ever the same as production  Seems to me that code which will execute during the compilation process that 
> depends on facets of the production environment will never compile ... or worse, compile  Or am I misapprehending? 
> package abc 
> { 
>     def = file::as_lines "abc"; 
>     .... 
> }; 
> Now imagine file "abc" exists in both production and compile environments, but with differing  Maybe not even a common 
>  This strikes me as a devil of a nasty bug to track down the first time. 
> Hue 



We're also going to need a tutorial on sharing by and by... 




2011-05-12 CrT Thoughts on Mythryl design: 

 o The pervasive use of synthesized text source code is a devastating 
   indictment of the functor system.  Clearly stronger medicine would 
   be highly useful  --- text source synthesis is utterly untypesafe. 

   Hacking at the the parsetree level via the "foo package .." hack 
   would be considerably more typesafe. 

   But it would be best to drive as much of this stuff down to as deep 
   a semantic level as practical.  Anything which can be done to allow 
   stuff to be done at the functor level instead of the parsetree or 
   sourcetext level would be an improvement. 



 o There's something deeply wrong with all the "sharing" stuff.  We're 
   supposed to be based on structural equality at the type level, but 
   somehow at the package level we've reverted to name equality.  Almost 
   any serious use of functors seems to immediately involve us in a web 
   of hand-maintained package-sharing relationships of exponentially 
   exploding complexity. 

   The diagnostics frequently say 

     foo::Bar != foo::Bar 

   which is intolerably obscure, but even if this were fixed the system 
   would still be a pain in the ass to maintain. 

   I think we need a semantics in which packages automatically retain 
   equality with themselves unless it is explicitly overridden, rather 
   than one in which every new reference to a package requires a web 
   of sharing declarations relating it to every other reference to that 
   package. 





Package names in api declarations should be 2-3 chars.  (This is the only 
case in which global-scope identifiers should be this short.) 


Should demonstrate the 

    ) 
    : (weak) 
    api { 
        include api Translate_Treecode_To_Machcode; 
        rewrite_mem_reg:  Bool; 
    }

technique for incrementally extending (subclassing) an API. 





Common Coding Myths: 

Your code should compile first time. 
==================================== 

A German prof once told me that -his- programmer's code compiled first time 
every time.  I hope he was fibbing  --- if not, that guy was wasting countless 
hours checking by hand for problems which the compiler can check for more 
quickly and accurately.  Never waste time doing by hand something which can 
be done cheaper and better by automation! 

When it runs, its done. 
======================= 

I remember coaching a computer science student through a homework 
problem once.  We only got as far as the first working version, and 
then he was off!  I was disappointed  --- I was only about one quarter 
of the way through my mental checklist of things worth learning from 
the problem. 

The first working prototype is just a successful exploration;  it is about 
the equivalent of getting a mobile home set up on the construction site to 
serve as headquarters during construction. 

The first working prototype is usually badly structured, missing all 
kinds of sanity checks, missing proper diagnostic reporting, missing 
routine logging, missing commenting, badly structured, with various 
ideas that didn't quite work out still lingering in the codebase, and 
often takes twice as much code as needed to do what it does, often in 
obscurely devious ways. 

The first working prototype gives you what you need to establish that 
the problem can be solved, and to start seriously engineering a -good- 
solution to the problem. 

The most important requirement of production code is that it be clear. 
Clear code that is broken can easily be fixed, but with obscure code that 
works it is often cheaper to throw it away and rewrite from scratch than 
to adapt it to changing needs. 



In 
    http://www.cs.nyu.edu/leunga/MLRISC/Doc/html/mltree-ext.html 
we read: 
    [...] Unfortunately, there is a cyclic dependency as MLTREEs 
    are defined in terms of sext, and { sext} is defined in terms 
    of MLTREEs. The usual way to deal with cyclic dependencies is 
    to use typeagnostic type variables. 

It might be worth clearly understanding that and thenw writing up 
a tutorial on the technique? 



Bringing your code up to code 
============================= 

We have building codes because expensive experience has demonstrated 
that while there are many ways to build, following certain rules saves 
time, money and lives in the long run. 

Programming is like that too.  There are many ways to code up a solution, 
but following certain rules saves you time and money in the long run.  If 
you are writing flight control software  --- or even furnace control software -- 
it might even save your life. 





Possible changes for next major release: 
========================================

 o For external consistency,  change \ddd escape in strings to conform with whatever C99 does. 

 o For clarity,               change TYPELOCKED to TYPELOCKED everywhere. 

 o For internal consistency, rework generic package argument syntax: 

    * When passing an argument by name, why can't we optionally just drop the parens entirely? 
      This is more consistent with fn invocation syntax etc. 

    * When passing an argument package by value, why don't we use { ... } instead of ( ... ) ? 

       -> This is more consistent because we use { ... } to delimit codeblocks, and this is a codeblock. 
       -> This is also more consistent because we use (...) for tuples, which are order-based, and {...} 
          for records, which are label-based  --- and the argument package is label-based more than order-based. 

 o Look at the where/sharing syntax and maybe rework it: 

    * Seems like one can write 

        package foo: Foo where this == that { ... }; 

      but that when doing types one has to retreat to  

        package foo: Foo where This == That = package { ... };  
     
    * I'm not clear where ``wheretype'' is needed. 

    * I think we could change 'sharing' to 'where' and eliminate one reserved word. 






The ''include'' statement is one of those tools which is good to have and even better not to use. 



Burying an exception-raise in your code is like burying a landmine 
in your flower garden.  There may be times and places where that 
is exactly the right thing to do, but in general it is something 
to be strenuously avoided. 

One of the nicest things about Mythryl code is that once it compiles, 
it often Just Works.  If it doesn't, an uncaught exception is likely 
to be the culprit.  The less use you make of exceptions, the more 
likely it is that your code will Just Work. 

Exceptions are especially bad at library interfaces;  there is an 
excellent chance that the client programmer will either forget to trap 
the exception or else not know how to handle it properly.  When exceptions 
-must- be used, it is much preferable  --- if practical  --- to generate 
and handle them within a single package, invisible to the rest of 
the world. 



IMPLEMENT THE API, THE WHOLE API, AND NOTHING BUT THE API. 

I sometimes see a package in which someone implements half the calls 
in an API and throws exceptions for the rest of them. 

DO NOT DO THAT! 

Jeez.  That's gross, that's crazy, that's evil, that's putting a gun 
in your mouth and pulling the trigger until it goes ``click''. 
There is a special circle of Hell reserved for people who do that. 

An api is a contract between package author and package client; 
code works only when both sides honor the contract. 

Advertising that you implement functions which you in fact do not is 
gross breach of contract.  It means code is going to compile fine and 
then fail at runtime, which is exactly what you do NOT want. 

If you are not going to implement a complete api, define a new api 
including just the stuff you're actually implementing, and use it 
instead. 



A MYTHRYL PROGRAMMING METHODOLOGY 

Knowing a language and its standard libraries is a good start, but 
producing solid usable programs on time takes a lot more.  That 
``lot more'' is your ``programming methodology''.  It is the mental 
bag of habits, techniques and tricks of the trade that you use to convert 
a vague idea in your head into running code. 

In many ways, your programming methodology is more important than your 
programming language.  Studies have repeatedly shown productivity 
factor differences of up a hundred between equally experienced 
programmers working under identical conditions.  A large part of the 
difference is the private programming methodologies they have 
developed.  If you adopt a good programming methodology, your 
programming life is likely to be happy and productive.  If you adopt a 
poor programming methodology, your programming like is likely to be 
miserable and unproductive. 

ML class languages are relatively new, dating back only to the 1970s, 
so effective programming methodologies are still evolving rapidly. 
Because they open new possibilities, they give you both better ways 
to solve problems, and better ways to shoot yourself in the foot. 

Mythryl, you might say, lets good programmers do better and bad 
programmers do worse.  It gives you enough rope to weave yourself a 
nice comfy hammock  --- or to hang yourself. 


 o Evolution not Revolution. 
 o Small transactions between correct codebase states. 
 o A fast regression suite run every compile cycle. 
 o Hardest part first. 
 o Don't debug  --- back up. 
 o Design-in transparency from the start: 
   * Invariant checkers for every major datastructure. 
   * Human display code for every major datastructure. 




Do not name packages or functions 'main'.  C programs need to 
have a 'main';  Mythryl programs do not, and having a bunch 
of packages and/or functions floating around your code library 
all named 'main' just sows confusion.  If your package or 
function unscrews the inscrutable, name it unscrew_inscrutable. 




Remove garbage code 
=================== 

Some beginners seem to have an odd ``every line of code is sacred'' 
mentality;  obsolete code gets left sitting around forever.  If 
code is no longer functional, and will not be in the foreseeable 
future  --- remove it!  The alternative is a codebase which grows 
ever more cluttered and unmaintainable.  If the code really represents 
a huge investment in blood, sweat, tears and brilliance (exceedingly rare), 
then set up an ``attic'' directory and save the stuff in there, out of 
the way. 



Don't build white elephants 
=========================== 

I keep running across apis with elements which are not used, 
which have never been used, and which most likely never will be used. 

I rip them out by the roots and burn them:  Simple apis make systems comprehensible. 

To control code bloat, the X Consortium eventually adopted the rule 
of adding no feature unless it made a major improvement in a real, 
existing application. 

That is an excellent rule of thumb:  Do not put anything in an api 
unless it makes a definite improvement in some real, existing client 
package. 




You may think that some of these rules are too obvious to be worth 
mentioning, but rest assured that I've seen production code grossly 
violating every single one of these rules.  SOMEBODY out there needs 
to hear this stuff! 



    Clarify globally; 
    abbreviate locally. 



The .api file should contain everything a client of the package needs to know about it; 
only the package maintainer should need to look inside the .pkg file. 



Should write a little tutorial on using disjoint_sets_with_constant_time_union 




Program Defensively 
=================== 

There is no silver bullet that automatically yields robust programs; 
you have to use your head, every time, all the time. 

By ``program defensively'' I am not primarily thinking of trivial stuff 
like thoroughly checking all input from users  --- that is obvious. 

What I mean is working smarter and designing bugs out of existence 
before they can happen. 

For example, suppose you're building a language runtime and the heap 
is kept in one contiguous memory region.  Every now and then you'll 
have to allot one twice as big and copy the contents over.[1] 

During the copy, all absolute pointers will have to be adjusted. 
The garbage collector does this anyhow, so you have the choice 
of ab/using the garbage colletor to do the copy, or writing a 
faster custom routine. 

Which should you prefer? 


    *    *    *    *    *    *    *    *    *    *    * 


Prefer ab/using the garbage collector to do the copy, because 
that is very well-tested code performing a notoriously complex, 
error-prone pointer-manipulation task.  A separate custom module 
doing much the same thing that is seldom actually used is likely 
to take years, even decades to be fully debugged, and to develop 
new bugs each time the heap structure and garbage collector are 
modified but someone forgets to properly update the separate 
double-the-heap-size module. 

By designing the latter nightmare module out of existence from 
the beginning, we kill dozens of subtle bugs off before they are 
even born. 

That's working smarter instead of harder! :-) 

More importantly, that is being bug-proactive throughout the 
design and implementation process, instead of just fixing bugs 
as an afterthought sort of thing when they actually happen. 

Bugs  --- human coding errors  --- are a fact of life that a good 
software engineer takes into account at all times. 

<i>Birth control is the best way to kill bugs!</i> 



SQL injection attacks are an example of working harder instead 
of smarter.  One idiot programmer once upon a time made the 
design blunder of passing SQL commands from client to server 
as an ASCII string instead of as a datastructure. 

This blunder guaranteed that any user-input validation oversight by 
any programmer in the system would turn into a security hole -- 
designed-in insecurity! 

The greater fools, of course, were the later programmers who instead 
of fixing this design blunder, just fixed the data validation problems 
one by one as they appeared, leaving the fundamental vulnerability intact. 

Lesson:  Don't kill that bug  --- kill that class of bugs! 







Note [1] 
======== 
Why do you double the heap size rather than (say) increasing it 
by 16MB?  Because if you double each time you guarantee that you 
only add a small constant overhead to overall memory allocation 
expense, but if you expand by 16MB each time you can slow down 
the program by an unlimited factor.  This depends ultimately 
on the familiar fact that the sequence 
   1/2 + 1/4 + 1/8 + 1/16 + ... 
converges to the finite value of 1 (think 0.11111... in binary == 1, 
just as 0.99999... in decimal == 1.0), but sequences like 
   1/2 + 1/3 + 1/4 + 1/5 + 1/6 ... 
diverges to infinity.  Knowing that is frequently critical to 
the working programmer! 











Thunks are the perfect code obfustication tool;  they can 
be used to make a static reading of the program text 
impossible by making the result of every function invocation 
sensitive to arbitrary details of previous dynamic execution. 
Their types are utterly uninformativer; they are opaque to 
runtime inspection; they can make the simplest computation 
an impenetrable enigma. 




Should add a 'lazy' tutorial to the docs.  The following tutorial 
is from the 2000 version of Bob Harper's book, the current version 
of which says it is under creative commons license with NO COMMERCIAL 
and NO DERIVED WORKS clauses.  So the tutorial would have to be 
rewritten from scratch to be used on the site, I think. 
The book URL is:  http://www.cs.cmu.edu/~rwh/smlbook/ 

I'd like to ease into this via my old interview-question: 
How many different n-node binary trees are there? 
Harper prefers to phrase it as ``how many ways are 
there to parenthesize a string of N multiplies'' and 
says these are the Catalan numbers. 

So we can start with a naive algorithm, then show how 
slow it is, then do memoization by hand, then show how 
Mythryl will do the memoization for you. 

And only then get into the lazy streams stuff. :-) 

---- Start of tutorial adapted from http://www.cs.cmu.edu/~rwh/introsml/core/lazydata.htm ---



As we saw earlier, a sumtype declaration is used to introduce a new 
type whose elements are generated by a given set of value 
constructors.  The value constructors may be used to create values of 
the type (by applying them to values of suitable type), and to 
decompose values of the type (by using them in patterns).  Value 
constructors, like all other functions in Mythryl, are evaluated eagerly, 
meaning that the arguments to the constructor are evaluated before the 
constructor is applied.  For example, to attach an element to the 
front of a list, we first determine the value of the element and the 
value of the list before building a new list with that element as head 
and that list as tail.  This policy is based on the intuitively 
appealing idea of a list as a kind of value that we manipulate by 
using the list constructors as functions and as patterns. 
 
An alternative is to view a data structure as being perpetually in the 
process of creation, rather than as a result of a completed 
computation.  According to this view a list may be thought of as a 
"partial", or "suspended", computation that, when provoked, computes 
just far enough to determine whether the end of the list has been 
reached, or , if not, to produce the next element of the list together 
with a suspended computation to compute the remainder of the list.  An 
added benefit of this viewpoint is that it is then possible to define 
infinite lists (better known as streams) that continually generate the 
next element, without ever reaching the end of the list.   This view 
of data structures as being in the process of creation conflicts with 
the eager evaluation strategy just described since under the eager 
approach all expressions are fully evaluated before they are used, 
whereas we would like to evaluate them only as much as absolutely 
necessary to allow the overall computation to proceed.  This is 
called, appropriately enough, lazy evaluation. 
 
Standard ML does not support lazy evaluation as a primitive notion; it 
can be implemented "by hand" using methods that are described later in 
these notes.   However, Mythryl does provide for lazy evaluation through 
an extension of the sumtype and recursive-val declaration forms.  We will 
illustrate these mechanisms by defining a type Stream(X) of streams of 
values of type X.  Based on the discussion above you might imagine that 
a stream is just an infinite list, but it is important to keep the two 
concepts separate.  Lists are eager types whose values are generated by 
finitely-many applications of ! to the empty list, [].   Streams are lazy 
types whose values are determined by suspended computations that generate 
the next element of the stream (and another computation to generate 
the remainder).  The two concepts are, and ought to be kept separate 
since they serve different purposes and require different modes of 
reasoning. 
 
First off, the lazy evaluation mechanisms of Mythryl must be enabled by 
evaluating the following declarations: 
 
    include package   lazy;       # lazy is from   src/lib/std/src/nj/lazy.pkg  
 
    set_control  "mythryl_parser::lazy_is_a_keyword"  "TRUE"; 
 
 
                                                 [ A faster but less general approach is to do 
 
                                                       controls::lazy_is_a_keyword := TRUE; 
 
                                                   I recommend sticking to the  set_control  interface. 
                                                                              --- Cynbe 
                                                 ] 
 
 
We may then define a Stream type as follows: 
 
    lazy Stream(X) = CONS (X, Stream(X)); 
 
The keyword "lazy" indicates that values of type Stream(X) are 
suspended computations that, when evaluated, yield a value of the form 
CONS (x, c), where x is a value of type X, and c is another value of 
type Stream(X), i.e., another computation of such a value. 
 
How might a value of type Stream(X) be created?  Since the description 
of values of this type we've just given is clearly "circular", we must 
employ a recursive value binding to create one.  Here's a definition 
of the infinite stream of 1's as a value of type int stream: 
 
    recursive my lazy ones = CONS ((1, ones)); 
 
The keyword "lazy" indicates that we are defining a value of a lazy 
type, which means that it must be kept as an incomplete computation, 
rather than fully evaluated at the time the binding is created.  What 
computation is bound to ones?   It's the computation that, when 
evaluated, yields CONS (1, ones), a stream whose head element is 1 and 
whose tail is the very same computation again.  Thus if we evaluate 
the tail of ones we will, once again, obtain the same value, and so on 
ad infinitum. 
 
How can we take apart values of stream type?  By pattern matching, of 
course!   For example, we may evaluate the binding 
 
    my CONS (h,t) = ones; 
 
                                [ Personally, in Mythryl I would write 
 
                                      ones ->  CONS (h, t); 
 
                                   --- Cynbe 
                                ] 
 
to extract the head and tail of the stream ones.  To perform the 
pattern match we must first force the evaluation of ones to obtain 
CONS (1, ones), then pattern match to bind h to 1 and t to ones.  Had 
the pattern been "deeper", further evaluation would be forced, as in 
the following binding: 
 
 
    my CONS (h, (CONS (h', t')) = ones; 
 
                                [ And of course here 
 
                                      ones ->  CONS (h, (CONS (h', t')); 
 
                                  ``Shortest expression first in a binary compound!'' 
 
                                   --- Cynbe 
                                ] 
 
 
To evaluate this binding, we evaluate ones to CONS (1, ones), binding 
h to 1 in the process, then evaluate ones again to CONS (1, ones), 
binding h' to 1 and t' to ones.   The general rule is pattern matching 
forces evaluation of partial computations up to the depth required by 
the pattern. 
 
We may define functions to extract the head and tail of a stream as 
follows: 
 
    fun shd (CONS (x, _)) = x;                          # "shd" == "stream head". 
    fun stl (CONS (_, s)) = s;                          # "stl" == "stream tail". 
 
Both of these functions force the computation of the stream when 
applied so that they may extract the head and tail elements.  In the 
case of the head element it is clear that the stream computation must 
be forced in order to determine its value, but a moment's thought 
reveals that it is not strictly necessary to force the computation of 
a stream to extract it's tail!  Why is that?  Since the tail of a 
stream is itself a stream, it may be thought of as a suspended 
computation.  But which suspended computation is it?  According to the 
definition just given, it is the suspended stream computation 
extracted from the second component of the value of the given stream. 
But another definition is possible: it is the suspended computation 
that, when forced, yields the second component of the result of 
forcing the stream computation.   Here's a definition: 
 
    fun lazy lstl (CONS (_, s)) = s;                # "lstl" == "lazy stream tail". 
                                                    # NB: CMU "sample code" file incorrectly omits the "lazy" in this fun.  
  
Here the keyword "lazy" indicates that an application of lstl to a 
stream does not immediately perform pattern matching (hence forcing 
the argument), but rather merely sets up a delayed stream computation 
that, when forced, forces the argument and extracts the tail of the 
stream. 
 
The behavior of the two forms of tail function can be distinguished as 
follows: 
 
    recursive my lazy s = { print "."; CONS (1, s); };  
    s' = stl s;                                              # Prints ".".  
    my CONS _ = s';                                          # Silent. 
 
    recursive my lazy s = { print "."; CONS (1, s); };  
    s'' = lstl s;                                            # Silent. 
    my CONS _ = s'';                                         # Prints ".". 
 
 
Notice that since stl immediately forces it's argument, the "." is 
printed when it is applied, whereas it is printed only when the result 
of applying lstl to an argument is itself forced by another pattern 
match. 
 
It is EXTREMELY IMPORTANT that you understand the difference between 
these two definitions!  To check your understanding, let's define a 
function smap that applies a function to every element of a stream, 
yielding another stream.  The type of smap should be 
 
   (X -> Y) -> Stream(X) -> Stream(Y). 
 
The thing to keep in mind is that the application of smap to a 
function and a stream should set up (but not compute) another stream 
that, when forced, forces the argument stream to obtain the head 
element, applies the given function to it, and yields this as the head 
of the result.  Here's the code: 
 
    fun smap f 
	= 
	loop 
	where 
	    fun lazy loop (CONS (x, s)) 
		= 
		CONS (f x, loop s); 
	end; 
 
Notice that we have "staged" the computation so that the partial 
application of smap to a function yields a function that loops over a 
given stream, applying the given function to each element.  This loop 
is a "lazy" function to ensure that an application of loop to a stream 
merely sets up a stream computation, rather than forcing the 
evaluation of its argument at the time that the loop is applied.  This 
ensures that we are as lazy as possible about evaluating streams. 
Had we dropped the keyword "lazy" from the definition of the loop, 
then an application of smap to a function and a stream would 
immediately force the computation of the head element of the stream, 
rather than merely set up a future computation of the same result. 
This would be a bit over-eager in the case that the result of applying 
smap were never used in a subsequent computation.  Which solution is 
"right"?  It all depends on what you're doing, but as a rule of thumb, 
it is best to be as lazy as possible when dealing with lazy types. 
 
To illustrate the use of smap, here's a definition of the infinite 
stream of natural numbers: 
 
    one_plus = smap (\\ n = n+1);  
  
    recursive my lazy nats = CONS (0, one_plus nats); 
 
It is worthwhile contemplating how and why this definition works. 
 
Now let's define a function sfilter of type (X -> Bool) -> Stream(X) -> Stream(X) 
that filters out all elements of a stream that do not satisfy a given predicate. 
Here's the code: 
 
    fun sfilter pred 
	= 
	loop 
	where 
	    fun lazy loop (CONS (x, s)) 
		= 
		if (pred x)  CONS (x, loop s); 
                else         loop s; 
                fi; 
        end; 
  
We can use sfilter to define a function sieve that, when applied to a 
stream of numbers, retains only those numbers that are not divisible 
by a preceding number in the stream: 
 
    infix my mod ;  
    fun m mod n =   m - n * (m / n);  
    fun divides m n =   (n mod m) == 0;  
    fun lazy sieve (CONS (x, s)) =   CONS (x, sfilter (not o (divides x)) s);  
 
We may now define the infinite stream of primes by applying sieve to 
the natural numbers greater than or equal to 2: 
 
    nats2 = stl (stl nats);          # Might as well be eager.  
    primes = sieve nats2; 
 
To inspect the values of a stream it is often useful to use the 
following function that "takes" n>=0 elements from a stream and builds 
a list of those n values: 
 
    fun take 0 _             =>  []; 
        take n (CONS (x, s)) =>  x ! (take (n - 1) s); 
    end;  
 
In addition to supporting demand-driven computation the lazy 
evaluation primitives of Mythryl also support memoization of the 
results of a computation.  The idea is that a delayed computation is 
performed at most once.  If it is never forced by pattern matching, 
then the delayed computation is never performed at all.  If it is ever 
forced, then the result of forcing that computation is stored in a 
memo pad so that if it is forced again, the previous result is 
returned immediately,without repeating the work that was done 
previously.  Here's an example to illustrate the effects of 
memoization: 
 
    recursive my lazy s =  CONS ({ print "."; 1; }, s);  
 
    s ->   CONS (h, _);                       # Prints ".", binds h to 1  
    s ->   CONS (h, _);                       # Silent, binds h to 1  
 
Replace "print "."; 1;" by a time-consuming operation yielding 1 as 
result, and you will see that the second time we force s the result is 
returned instantly, taking advantage of the effort expended on the 
time-consuming operation induced by the first force of s. 
 


As a programmer, your Great Enemy is -- Complexity.

Your best weapons against the Great Enemy are:

  o Reduction to a normal form.

    Shakespeare spelt his name seventeen different ways over his career.
    If all of those were still in use, googling for him would be a nightmare.
    Fortunately the literate world has settled on a single standardized
    spelling for his name, eliminating that nightmare. 

    Every time you standardize on one of two possibilities in your coding,
    you cut in half the number of possibilities you have to cope with when
    searching or modifying the codebase.

    Do that ten times, and you've made many of your maintenance tasks
    one thousand times easier!

    Some of your best friends as a programmer are:

       * Standardized spelling.
       * Standardized vocabulary.
       * Standardized indentation.
       * Standardized commenting and cross-referencing conventions.
       * Standardized sourcefile layout.
       * Standardized datastructures and algorithms for common tasks.
         Red-black tree or 2-3 tree? Pick one and stick to it.

    In general, what matters is that you have a standard, not which standard
    you pick.  This is why indentation style flamewars (for example) are interminable:
    Having a one is important but which one -- not so much.


 o  System factorization into small packages with simple, narrow interfaces.

    To be humanly comprehensible code packages should run about 100 -> 5,000 lines.
    Cleanly factoring software systems into such packages is a black art -- and the
    most important skill a software engineer can possess.

    ``Simple, narrow'' must be understood to apply to both syntax and semantics
    of the package interface.  A small number of exported (and imported!) functions
    is good;  it is also good if they behave in simple, obvious ways.  Obscure
    side-effects on hidden state are doubleplus ungood, for example.

    (Some packages really need to be huge and ugly, for one reason or another.
    Stanford prof David Cheriton calls them ``warthogs''.  You can still veil
    the ugliness of a warthog module behind multiple simple interfaces, each
    addressing one part of its its functionality.  Mythryl makes it easy to
    seal a package with multiple APIs, yielding multiple views of it for use
    by different kinds of code clients.  Cheriton refers to these simple views
    as ``trampolines'', and the combines use of the two as the warthog/trampoline model.)


 o  Conceptually self-sufficient packages:  Ideally everything needed to understand a package
    should be in it or in the APIs of the packages it references, and the sum total lines of code
    in the module plus the APIs it references should be less than 10,000 lines of code.

    10,000 lines of code is the most that a human can reasonably grasp at any one time;
    if working on any particular package requires more than that, you've probably got
    a maintenance nightmare on your hands and a perpetual source of bugs.

    (Yes, there are obvious exceptions, starting with automatically synthesized packages.)


 o  Principle of Least Surprise.  Nine times out of ten, when you need to pick between
    several design alternatives, the most boringly obvious -- the least surprising -- solution
    is the one you want.  In programming, ``clever'' is a dirty word.  Any time you find yourself
    having to devote a paragraph of comments to a line of code, it is time to ask yourself if
    there isn't a better design solution available.  

    (Yes, there is still the tenth time, when the cleverness is worth the price.  As an engineer
    you have to use your head -- there are no silver bullets that solve all the problems for you.)


 o  Incremental evolution of an initially simple, working system -- evolution not revolution.
    Almost without exception, every complex working system grew from a simple working system.

    Rewriting a      10,000-line working system from scratch fails about  1% of the time.
    Rewriting a   1,000,000-line working system from scratch fails about 50% of the time.
    Rewriting a 100,000,000-line working system from scratch fails about 90% of the time.

    (One of many problems with the latter case is that the rewrite team is almost always
    smaller than the maintenance team working on the existing system, and consequently
    the replacement system never catches up to the existing system in functionality.)

    Incremental evolution of a software system preserves and propagates the implicit
    knowledge stored in the codebase and gives all the parts of the system time to
    anneal together into a well-balanced whole.  In essence short-range mutual accomodations
    between packages while the system is small evolve into long-range coherent structure as
    the system evolves.

    Successful evolution does demand a culture in which each programmer takes care that
    each change leaves the system better.  A culture in which quick-and-dirty local fixes
    continually degrade global system integrity -- in which entropy is allowed to steadily
    dissolve codebase order into chaos -- results not in codebase evolution, but rather in
    codebase death by a thousand hacks.




From: cynbe@mythryl.org
To: mythryl@mythryl.org
Subject: Speed tip: duplicate inlinable fns in-package.
bcc: cynbe-cc
--text follows this line--

For the benefit of any beginners on the list, let me
start by repeating the standard optimization mantra:

 o Premature speed optimization is a beginners disease
   and the source of a gawdawful percetage of bugs and
   readability problems:

       Never optimize for speed until you have a working
       problem with proven performance problems.

   90% of programs are more than fast enough without any
   speed hacking.

 o The first line of defense against performance problems
   is to pick good algorithms.  For example, if you picked
   a slow (O(N**2)) sort algorithm, no amount of low-level
   linear speedups is going to save you.

 o You don't make a slow program fast by doing low-level
   speed optimizations pervasively through the code.  You
   identify the tiny hotspots where the program is spending
   all its time and you optimize them to the eyebrows; you
   leave the rest of the program simple, robust and readable.


Ok, with that out of the way:  I was recently reading

    Lambda-Splitting: A Higher-Order Approach to Cross-Module Optimizations.
    Matthias Blume and Andrew W. Appel,
    Proc. ACM SIGPLAN International Conference on Functional Programming (ICFP '97),
    pp. 112-124, June 1997.
    http://www.cs.princeton.edu/~appel/papers/inlining.ps

which describes the SML/NJ (and thus Mythryl) cross-package inlining
facility.  In the paper they try to put a good face on things, but
the bottom line is that they put a lot of analysis, design time and
implementation effort into building a cross-package inlining facility
for the compiler, and when they turned it on -- everything got SLOWER.

Which probably explains why it is turned off by default
in the shipping compiler.  :-)

So working out an actually effective way of using this facility would
be a nice project for someone at some point.

In the meantime, what I see the Fellowship of SML/NJ people doing is
duplicating standard library functions in-package when speed it a
consideration, since in-package inlining works quite well, thankyou.
In practice this seems to be mainly little convencience functions
from packages like   src/lib/std/src/list.pkg

So that is today's Mythryl Hint:  If speed matters, consider
duplicating in-package any critical routines which you want
inlined.

(This would appear to be a practical argument in favor of having
a C-style #include statement that textually includes stuff in a
.pkg file.)

Life is Good!

  -- Cynbe




Related packages and functions should look related.

One way to do this is to use a word in common in the function
names to tie them together -- a tieword.  For example, the original
Mythryl interthread communication mechanisms had no lexically obvious
relationship.  Rewriting with the word 'mail' as the tieword established
an immediately obvious relationship between

    mailop
    mailslot
    mailqueue
    mailcaster
    maildrop
    onetime_maildrop

Another way to make related functions look related is to use parallel name structures.
For example, in

		    stipulate
			my fun_id__to__codelabel__hashtable:   iht::Hashtable ( lbl::Codelabel )
				                           =   iht::make_hashtable  { size_hint => 32,  not_found_exception => LABEL_BIND };
		    herein
			fun_id__to__codelabel =   iht::get   fun_id__to__codelabel__hashtable;
			note_function_label   =   iht::set   fun_id__to__codelabel__hashtable;
		    end;

the two functions fun_id__to__codelabel and note_function_label are closely
related, but they don't look closely related.  The reader will have to slow
down and think a bit to realize that they are a pair, and will have to take
time to memorize that fact before reading the rest of the file.

A better pair of names is

		    stipulate
			my fun_id__to__codelabel__hashtable:   iht::Hashtable ( lbl::Codelabel )
				                           =   iht::make_hashtable  { size_hint => 32,  not_found_exception => LABEL_BIND };
		    herein
			get_codelabel_for_fun_id =   iht::get   fun_id__to__codelabel__hashtable;
			set_codelabel_for_fun_id =   iht::set   fun_id__to__codelabel__hashtable;
		    end;

Now the relationship between the two functions jumps off the page at the reader.



Should have a section explaining the naming conventions for number and vector packages,
in particular that the 'two-word-int.pkg' style ones are agnostic (and vary in size)
between 32-bit and 64-bit architectures, while the 'one-byte-int.pkg' style ones have
identical sizes on 32- and 64-bit architectures.



Should have a a tutorial on memoize.pkg and probably on 'lazy' keyword.  (If we do the
latter, we should make it permanently 'on' rather than having to flip a switch.)




Do we have a tutorial anywhere on loop idioms?  'apply' and 'map', obviously, plus					# Should we define   apply'   and   map'  that take args in reverse order...?
the outer-wrapper/inner-loop idiom like from  src/app/makelib/compile/compile-in-dependency-order-g.pkg
(a more concise tutorial example can be written):

					stipulate
					    # A helper fn to remove all PRE_COMPILE_CODE entries
					    # from a list of declarations and return the remaininng
					    # declarations plus the extracted strings.
					    #	
					    # The arg pattern is (input, output, output):
					    #	
					    fun split ([], declarations, pre_compile_code_strings)						# Done --
						    =>												# return 
						    ( raw::SEQUENTIAL_DECLARATIONS (reverse declarations),					# declarations with PRE_COMPILE_CODEs removed,
						      (reverse pre_compile_code_strings)							# plus the PRE_COMPILE_CODE strings.
						    );

						split (raw::PRE_COMPILE_CODE string ! rest,  declarations,  pre_compile_code_strings)		# Add 'string' to pre_compile_codes and continue.
						    =>
						    split (rest,  declarations,  string ! pre_compile_code_strings);
						    	
						split (                      other ! rest,   declarations,  pre_compile_code_strings)		# Add 'other' to result declarations and continue.
						    =>
						    split (rest,  other ! declarations,  pre_compile_code_strings);
					    end;
					herein
					    # Wrapper fn which removes PRE_COMPILE_CODE entires from
					    # a raw::Declaration list, returning both the filtered list
					    # and also the removed strings:
					    #
					    fun split_off_pre_compile_code (raw::SEQUENTIAL_DECLARATIONS declarations)
						    =>
						    split (declarations, [], []);

						split_off_pre_compile_code other								# This case can't happen -- parse_all_declarations_in_file () always returns
						    =>												# raw::SEQUENTIAL_DECLARATIONS in   src/lib/compiler/front/parser/main/parse-mythryl.pkg 
						    (other, []);
					    end;
					end;



Mythryl Pragmatics:

 "Experience keeps a dear school, but fools will learn in no other."
                                  -- Benjamin Franklin



Quotes looking for a home:

 "As a rule, software systems do not work well until they have been used, and have failed repeatedly, in real applications."
                                  -- Dave Parnas 

 "Computers are like Old Testament gods; lots of rules and no mercy. 
                                  -- Joseph Campbell 


 "Computing is not about computers any more. It is about living."
                                  -- Nicholas Negroponte 

 ``I am regularly asked what the average Internet user can do to ensure his security. My first answer is usually 'Nothing; you're screwed'.'' 
                                  -- Bruce Scheneier 

 "I happen to think that computers are the most important thing to happen to musicians since the invention of cat-gut which was a long time ago. "
                                  -- Robert Moog 

 'In the practical world of computing, it is rather uncommon that a program, once it performs correctly and satisfactorily, remains unchanged forever."
                                  -- Niklaus Wirth 

 "Many of our own people here in this country do not ask about computers, telephones and television sets. They ask - when will we get a road to our village."
                                  -- Thabo Mbeki 

 "Part of the inhumanity of the computer is that, once it is competently programmed and working smoothly, it is completely honest."
                                  -- Isaac Asimov 




Write prettyprint and sanity-check functions proactively.
Every complex datastructure should have prettyprint and sanity-check functions
written at definition time -- don't wait until you have an unexpected bug
on a tight deadline.



There's a fundamental(?) problem with simple OOP losing polymorphism.
If we have a function

    make_thread: (Void -> X) -> Thread(X);

then make_thread is polymorphic and we can create threads returning different values.
But if we create a thread manager

    Thread_Boss(X)
	=
	{ make_thread: : (Void -> X) -> Thread(X),
	  ...
        };

    make_thread_boss: Void -> Thread_Boss(X);

with the idea of doing

    
    thread_boss = make_thread_boss ();

    thread1 = thread_boss.make_thread  fn1;
    thread2 = thread_boss.make_thread  fn2;
    ...

then thread1, thread2 ... will all be constrained to have the same return type,
basically because X is fixed when we do make_thread_boss ().




====================================================================================
====================================================================================
Time profiling:   I'm sure I posted a little tutorial on time profiling to the mailing
list, but I see no trace of such a tutorial in the .tex or src/A.*.OVERVIEW files;  it
needs to be fished out of the mailing list and incorporated in the tutorials.



====================================================================================
====================================================================================
Multithreaded programming:  I don't think I've ever posted any kind of tutorial on this...?
Clearly a lacuna of Pacific Ocean proportions. :-)




====================================================================================
====================================================================================
AVOID EXCEPTIONS LIKE THE PLAGUE!!

When a Mythryl program fails at runtime, the cause probably nine times out of ten
is an uncaught exception.  An exception is like a GOTO only worse, because it can
cross package boundaries, while a GOTO at least just screws up local logic.  When
you throw an exception, you are shouting the top of your lungs, "Hey bugs!  Come
and roost in my code!"

Every exception you throw is a landmine buried in the code for future maintainers
of the package to step on.

Worst of all are exceptions thrown by library code that need to be caught by client
code written by other people.  When you throw an exception like that you are shouting
at the top of your lungs, "Hey bugs!  Come and roost in HIS code!"  That's just not
civilized behavior --- that is downright malicious.  If your library function may
fail, it is much better to return NULL on failure and THE(value) on success.  If the
client programmer then wants to throw an exception on NULL that is not your fault,
and at least the danger of the exception is then visibly marked in the client code.


====================================================================================
====================================================================================
HOW TO WRITE CODE THAT "JUST WORKS(TM)"

 o Simplify, simplify, simplify!  Rare performance-critical hotspots aside,
   the simplest solution it usually the best, so long as you don't turn an
   O(n*logn) solution into an O(N**2) solution in the process of simplifying.

   Complexity is the programmer's Great Enemy.

   Complex code is the favorite breeding ground of bugs.

   There is no such thing as code which is too simple or too clear.

   Never be satisfied with code merely because it works;  constantly
   watch for opportunities to make the code simpler or clearer, and jump
   enthusiastically at every such opportunity.

   Commenting problematic code is good, but avoiding problematic code completely
   is even better.  If you have to explain the dangers of a particular bit of
   code, that is a strong hint that it is time to redesign or reimplement it.


 o When you can't simplify -- factor!  If there is a particular problematic
   gotcha in the code that simply will not go away, try to at least isolate
   that aspect of the codein its own package, with as little other stuff in
   that package as possible, so that the gotcha can be understood in as simple
   a setting as practical -- so that related bugs have as little undergrowth
   to hide in as humanly possible.


 o Avoid exceptions!  Avoid exceptions!  Avoid exceptions!  Exceptions are
   nonlocal GOTOs, and introduce all the logic risks for which GOTOs are notorious.
   Sometimes exceptions are exactly what you need -- that is why the language
   has them -- but any time you can reasonably avoid them, you are doing yourself
   a big favor.

   Whenever reasonably possible, return Null_Or(Foo) instead of throwing an
   exception because you cannot return a valid Foo.  Returning Null_Or(Foo)
   forces calling code to properly handle the case;  throwing an exception
   makes it easy for calling code to forget to handle it properly and fail
   miserably in the field when least expected.

   When you definitely need an exception, try to keep it as local as possible,
   definitely confined to a single package if at al possible.


 o Avoid mutable datastructures!  Again, sometimes a refcell or mutable vector
   is exactly what you need, but whenever you can reasonably substitute a
   red-black tree for a vector or hashtable, or can reasonably copy-with-changes
   a record rather than having a refcell in it, you are doing yourself a big
   favor in terms of expected debugging time.

   Once again, when you do need mutable store, keep it as local as possible.
   Arguments to a tail-recursive function are the safest mutable values.
   A refcell visible only within the body of a single function are next
   safest.  If you cannot keep your mutable datastructure confined to
   a single function, see if you cannot at least keep it confined to a
   single package -- the smaller the better.


 o Use enumerated sumtypes instead of string or int constants,
   use 'case' statements (or the implicit case stements of function syntax)
   together with recursion to process them, and avoid wildcard cases.
   When you do this the compiler can warn you whenever you leave out a case
   or have a redundant case -- killing many bugs before they are born.

   If you avoid wildcard cases (_) then every time you add a new element
   to a sumtype, the compiler will tell you about every place in the
   program where you need to add code to handle it.  If you habitually
   use wildcard cases in your code, you're on your own -- a fruitful source
   of silent bugs.

   Whenever reasonably possible avoid strings and text in favor of more
   structured sumtypes with clearer semantics.


 o Buy typesafety by replacing tuples with records.  (0.3, 0.7, 0.4)
   could be an x-y-z point or a red-green-blue triplet;  if you have both
   circulating in your code, it is only a matter of time until one gets
   used as the other, introducing a bug.   With { x => 0.3, y => 0.7, z => 0.4 }
   vs { reg => 0.3, green => 0.7, blue => 0.4 } those bugs are prevented
   by design rather than by implementation -- always better!

   (By the way, you cannot use "efficiency" as an excuse for sloppiness in
   this case -- records and tuples compile into exactly the same code, because
   the first thing the Mythryl compiler does with tuples is to convert them
   into records.)


 o Avoid explicit loops in favor of 'map' and 'apply'.  Off-by-one errors in
   loops are a favorite breeding ground of bugs.  By using 'map' and 'apply'
   prevent these bugs from forming -- always better than fixing them.


 o Make a point of using the Mythryl type system creatively to prevent bugs.
   Don't just grind out obvious type declarations -- invest substantial
   skullsweat dreaming up interesting new ways of putting the Mythryl type
   inference and typechecker to work for you to prevent bugs from being born.
   They are powerful tools -- but only help you if you USE them!  A "bulletproof"
   vest can save your life -- but only if you wear it.  It doesn't do any good
   hanging in the closet.

   For starters, make sure you totally understand and are completely comfortable
   with standard Mythryl "phantom type" techniques.

   If you make every value in your program a string, the type system is helpless
   to prevent you from using values inappropriately -- you might as well be
   writing in Tcl.  Your code will become a garden in which bugs will multiply
'  like weeds after a spring rain.

   Using bare integers for everything is nearly as bad.

   Use enumerated sumtypes whenever reasonably possible.

   Even when you need integers, use type abstraction or sumtype wrappers
   to prevent integers meaning different things from being mixed inappropriately.  

   Same thing with floats -- don't just use bare floats for both yards and
   meters, hide them behind Yard and Meter abstract types, or wrap them in
   YARD and METER constructors.  Type safety is almost always more important
   than any minor efficiency issues introduced by wrapping.



In general recursive rewriting of recursive datastructures defined by
recursive types is where Mythryl shines brightest.  So long as you can
stick to this style of computation, life will be fine all the time. The
farther you wander from this suny heartland, the more debugging pain
and conceptual murkiness you can expect to experience.


[This one maybe belongs in another section:]

 o Explain in clear comments why each operation is needed whenever there is
   the slightest mystery about it.  Clearly documenting WHY something is
   being done is much more important than documenting HOW it is being done.
   HOW can always be reconstructed, but once WHY is forgotten there is no
   easy way of recovering it.






Understanding Largescale Software Development
=============================================

In the 1970s Niklaus Wirth popularized the idea of "topdown development by
stepwise incremental refinement".

The basic idea was, starting with a vague idea like "let's write a compiler"
to begin by refining it into a sequence of slightly more specific steps like

    compile_sourcefile ( filename ) {
	#
        filehandle  =  open( filename );
	#
        tokenlist   =  lex( filehandle );
	#
	parsetree   =  parse( tokenlist );
	#
	typecheck( parsetree );
	#
	treecode    =  parsetree_to_treecode( parsetree );
	#
	treecode    =  optimize_treecode( treecode );
	#
	basicblocks = treecode_to_linear_code( treecode );
	#
	basicblocks = optimize_linear_code( basicblocks );
	#
	objectcode  = do_instruction_selection( basicblocks );
	#
	write_to_disk( objectcode );
     }

and then continue by recursively refining each of those operations into a sequence of
suboperations until finally one reached the level of statements directly implemented
by the programming language and/or standard libraries, at which point the program was
ready to run.

Wirth was only interested in programs of about five thousand lines or less, and on this
scale his approach can actually work quite well.

Unfortunately, every time a problem gets ten times bigger it becomes qualitatively
different;  one cannot develop programs of five million lines of code just by
scaling up the techniques used to develop five thousand line programs.

On this scale, problems with topdown development by stepwise refinement include:

  o It only addresses writing new code from scratch, whereas at least 80% of real
    coding effort is development of existing software systems.

  o It produces a huge mass of untested code which all gets compiled together for
    the first time; resulting in a horrendous debug problem.  Unit testing can help
    somewhat, but the problem remains that at the integration stage a large number
    of interacting bugs must be dealt with simultaneously, which is a nightmare.

 o  The low-level code tends to lack integrity and reusability because it was developed
    in and for one specific code context and lacks natural portability to other code
    contexts.


An alternative model of software development (used for example by the MIT Programmer's
Apprentice project in the late 1970s and early 1980s) is to think of software development
as a sequence successive transforms to a conceptually complete software system.  Each
transform starts with a working system, breaks it transitionally, and then restores it
to working condition when done.

This model has in particular the advantage of applying to the maintainance phase of
software systems, where most programming actually takes place.  It also provides a
better match to the way most successful programs are developed in practice, which is
to get a very simple system working and to then incrementally add features per plan
and/or user request.

When using this model of software development it is natural to set up an automated
test suite early and to run it after each codebase transformation, to provide some
confidence that the transformation has indeed restored the codebase to fully functioning
form upon completion.

As observed by the Programmer's Apprentice project, it is useful to distinguish two
kinds of codebase transformations:

  o "Sideways" (equivalence) transforms, which leave unchanged the externally visible behavior of the system.
  o "Forward" transforms, which change the visible behavior to be closer to that desired.

Realistic software development typically alternates the two. "Forward" transforms are
what we want; we do them whenever they are directly possible.  However, often internal
structure problems prevent the desired "forward" transform from being performed.  In
that case we must first apply one or more "sidewase" trasforms to internally restructure
the codebase, as preparation for the desire forward transform.

Thus, we are led to think of software development planning in terms of selecting a
set of transforms which lead from the existing codebase to a codebase with the desired
new behaviors via a series of transforms each of which are short, practical, and restore
pre-existing functionality upon completion.

Questions which arise naturally in this model include:

  o How big should a transform be -- how many lines of code should be changed?

  o How does one select the next transform to apply?




How big should a transform be?
==============================

A transform must be big enough to restore full system functionality;
this usually puts a sharp lower bound on transform size.  In practice
this minimum transform size is usually the best size to use:

     Apply to the codebase the minimum-size transforms consistent
     with restoring full system functionality and making forward
     progress.

From a more theoretical perspective, the considerations are:

  o Debugging is much easier when there is only a single fault in the
    system.  Consequently the transform size should be small enough
    that on the average it rarely introduces more than one new bug
    into the codebase.  (Here we are primarily concerned with runtime
    bugs;  bugs caught at compiletime are normally localized accurately
    and easy to fix individually.)

  o The main purpose of terminating a transform and running a
    build-test cycle is to catch and correct programming errors.
    If transforms become so small that bug probability is negligible
    and built-test time dominates actual programming time, programming
    productivity is lost.  So from this perspective the rule of thumb
    is to make each transform big enough that one spends at least as
    much time programming as one does running the compiles and tests.




How does one select the next transform?
======================================

Often a project only involves one transform:  A simple bug needs to
be fixed, or a function needs a small increment of added functionality.

But in general a large project may involve substantial numbers of
codebase transforms for completion, and on must face the question
of how to select and order them, and how much forward planning to
attempt.

Hiearchy is ever our friend when taming complexity, so it is natural
to think in terms of some hierarchy of conceptual transforms which
then get broken down into successively smaller and more concrete
transforms until finally we reach ones which can reasonably be
actually directly performed upon the codebase.

One can imagine attempting to map out in advance the complete sequence
of transforms to be performed in a software project, but this is not
very realistic -- it would require a lot of bookkeeping, and one always
learns as one goes along, invalidating previous planning, so attempting
to be heroically complete in forward planning is usually a waste of time
in practice.

Conversely, without any planning process it may not be at all clear
how to make any progress at all on the problem, and easy to in essence
wander in circles transforming the codebase in ways which are not very
useful or perhaps in fact entirely useless or even counter-productive.

Thus, we require something like a happy medium in which we perform enough
forward planning to have a reasonably clear conception of why we we applying
a particular transform, without getting so bogged down in planning that we
never get any actual coding done.

My mental model for this process is what might be called successive midpoint
selection:

Suppose we want to guide a covered wagon from Detroit to San Francisco.

We know that the biggest problem is crossing the Sierra mountain range, and
that it can only be reasonably be crossed via one of a very few passes.  So we
begin by picking one of those passes.  That divides the original problem
into two subproblems:  Detroit-> Our_Pass and Our_Pass -> San Francisco.

Now, we can afford to delay worring about the Our_Pass -> San Francisco leg
until we summit the pass, or at least until we get close, so our real concern
at this point is how to get from Detroit to Our_Pass.

Again, we know that there is one overriding problem to be solved here, which
is crossing the Mississippi [...]

In essence here we are proceeding by recursive subdivision.  If we want to get from A to Z


            A------------------------Z

we first find a point M about midway between

            A-----------M------------Z

and narrow our focus to "How do I get from A to M?"  Repeating the procedure

            A-----G-----M------------Z

successively reduces the problem to "How do I get from A to G? and then

            A--D--G-----M------------Z

"How do I get from A to D?" and finally

            AB-D--G-----M------------Z

"How do I get from A to B", which with luck we can solve directly via a
concrete coding transform.  If our AB transform succeeds we will then
be faced with the problem

             B-----------------------Z

"How do I get from B to Z", and with luck can just re-use most of our
previous plan

             B-D--G-----M------------Z

with a new final midpoint C

             BCD--G-----M------------Z

to select our next codebase transform BC with only O(1) planning overhead
for the next step.


For a project codebase transform distance of N this general approach
gives us O(log(N)) advance planning effort before starting on our first
concrete individual codebase transform, and a worst-case overall project
planning cost of  N * O(log(N)) == O(N*log(N)), which is quite tolerable.

Drawn on a simple line segment as above, the planning process looks trivial;
I hope our previous example of crossing the country by covered wagon shows
something of the actual analytical complexity which is likely to occur at
each step of the recursive subdivision process.


Let me emphasize that at no point do we try to map in advance the entire project
transform sequence

            ABCDEFGHIJKLMNOPQRSTUVWXYZ

That would waste O(N) effort on advance planning before we did the first concrete
AB code transform, and most likely by the time we got to D or so (and almost
certainly by the time we got to M or so) we would have encountered enough unexpected
difficulties and learned enough more about the problemspace to invalidate the last half
or so of our analysis.  Such overplanning is a recipe for "analysis paralysis", with a
potential worstcase planning overhead of O(N**2) -- intolerable in any practical setting
lacking Pentagon funding.

On a single-programmer project on a codebase of 100,000 lines or so one can
usually do the entire planning process in one's head on an informal
basis; as one scales up to 1,000,000 and 10,000,000 and 100,000,000
lines of code and correspondingly larger project teams, the planning
process gets harder and has to become correspondingly more elaborate
and more formal.

The real lesson here is not that one should develop elaborate planning
procedures, but rather that one should avoid megaline projects like
the plague.  Linux and Windows implement similar functionality, but
Linux does it largely with a large collection of one- and
two-programmer projects in the 100,000 lines of code range, while
Windows does it with enormous monolithic codebases.  This is an illustration
of the truism that software organization mirrors the the structure of the
human organization creating it.  It is also proof positive that humongous
codebases are a choice, not a requirement.

Programming teams have terrible scaling performance because

 1) The bandwidth inside one human head is orders of magnitude higher than
    the bandwidth between any two human heads;

 2) The amount of communication required to bring all available expertise
    to bear scales as O(N**2) in the size N of the programming team:
    Big (human) programming teams are inherently dysfunctional.

In short, there is a huge win in breaking the system up into codebases small
enough to fit in one human head -- which experience shows to be roughly
100,000 lines of code -- and working very hard to keep them sufficiently
decoupled via stable interfaces to allow those codebases it evolve in effective
isolation.

It is much better to devote time, money and effort to achieving this factorization
than to attempting to beat to death with dollars and programming hours the inherent
O(N**2) scaling problems of large codebases and development teams.

    Large codebases and large development teams
    are not the solution;  THEY ARE THE PROBLEM.

(This is the fundamental flaw in the "Chief Programmer Team" concept proposed
by Fred Brooks in his classic "The Mythical Man-Month":  The solution to 
codebase and development team scaling problems is not to try to cram slightly
larger problems into one human head by using three people to do the work of one,
but rather to do a better job of dividing the problem into parts each of which
is comprehensible to a single human brain without recourse to extreme measures.)




Heap debugging
==============

Should probably have a section on this.  Here's the listpost I used to announce it:

From: cynbe@mythryl.org 
To: mythryl@mythryl.org 
Subject: FYI: Technotrivium: Mythryl heap inspection. 
bcc: cynbe-cc 
--text follows this line-- 
 
I decided to stop and build a bigger Heisenbug gun. :-) 
 
One of the biggest gaps in the SML/NJ codebase is a near-total lack 
of tools to display the Mythryl heap for human inspection.  I've 
been letting this lie because I've been doing critically needed 
work on a scariest-project-first basis and filling this lacuna 
has not previously risen to the top of my job queue. 
 
Now it has, so I've established 
 
    src/c/heapcleaner/heap-debug-stuff.c 
 
to contain the main logic to dump the Mythryl heap contents to disk 
in text form.  (I resist saying "human-friendly";  megabytes of hex 
dump are not very human-friendly no matter how nicely formatted. 
To tell the truth, it reminds me unpleasantly of the hex dumps from 
the UW campus CDC6400 back in 1974 when I was learning to code via 
Hollerith cards... :-) 
 
This C-worldl functionality gets exported via 
 
    src/c/lib/heap/libmythryl-heap.c 
 
to emerge in the Mythryl world via 
 
    src/lib/std/src/nj/heap-debug.api 
    src/lib/std/src/nj/heap-debug.pkg 
 
This stuff will probably evolve pretty quickly for awhile, so I 
won't document here the contents of the latter two files;  take 
a peek in them when/if you get interested. 
 
There is an overview of the Mythryl heap datastructures in 
 
    src/A.HEAP.OVERVIEW 
 
-- that is the best entrypoint into this stuff. 
 
I've implemented this stuff at the C level partly because the 
relevant code and datastructures all live in the C world, but 
more importantly because the motivating idea is to debug heap 
corruption due to compiler bugs or whatever by doing 

    heap_debug::dump_foo(); 
    suspect_operation(); 
    heap_debug::dump_foo(); 
 
and then 'diff'-ing the two dumpfiles to see exactly what changed. 
 
For this to work well, it is important that dump_foo() disturb 
the Mythryl heap as little as practical.  Mythryl code inherently 
modifies the Mythryl heap continually, whereas C code has no inherent 
effect on the Mythryl heap, so C is our way out of Heisenberg Uncertainty 
-- it lets us observe without changing. :-) 
 
At present heap_debug::* dump the contents of the Mythryl heap pretty 
much entire, but do not dump all the internal heap-management datastructures, 
such as the BIBOP (BIg Bag Of Pages) stuff.  This is pure expediency -- 
code-to-need. 
 
All of this is stuff that the normal application user should seldom need 
to know or think about;  it becomes important only in the presence of 
implementation bugs.  (As an application programmer, there is some benefit 
to understanding the heap and its algorithms from the point of view of 
understanding more accurately the space/time costs of the code you 
write: Answering questions like "How much space does a string really take?" 
"How much garbage collection overhead is involved in updating a 
refcell?") 
 
I must say that after years of puzzling out the code maintaining the
Mythryl heap, it is a great pleasure to actually -see- the heap contents.
It is a bit like the difference between reading about Middle Earth and
actually seeing it up on the big screen. :-)

I think I make take a quick stab next at hammering out a simple x86
disassembler;  being able to see the compiled code as assembly instructions
is the last big missing piece of this heap-inspection puzzle at the moment,
albeit not directly relevant to my immediate needs.  I started writing one
in Mythryl:



Complexity is the Great Enemy
=============================

Space and time efficiency are minor issues.  They are dominated
by local hotspots amenable to local solutions.  Complexity is
the Great Enemy.  It is a global problem which pervades the program
codebase and defies local solutions.  It can only be dealt with by
an equally pervasive engineering solution.

