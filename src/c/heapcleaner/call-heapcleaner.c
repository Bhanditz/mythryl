// call-heapcleaner.c
//
// The main interface between the heapcleaner
// ("garbage collector") and the rest of the
// run-time system.
//
// We may be manually invoked from the Mythryl level via
//
//     src/lib/std/src/nj/heapcleaner-control.pkg
//
// We may be automatically invoked via the CHECKLIMIT macro in various allocation fns in
//
//     src/c/machine-dependent/prim.intel32.asm
// 
// and thence via   system_run_mythryl_task_and_runtime_eventloop/REQUEST_CLEANING   in
//
//     src/c/main/run-mythryl-code-and-runtime-eventloop.c
//
// to our   call_heapcleaner   entrypoint.
//
// More generally, we may be invoked via the heaplimit checks and heapcleaner calls generated by
//
//     src/lib/compiler/back/low/main/nextcode/pick-nextcode-fns-for-heaplimit-checks.pkg
//     src/lib/compiler/back/low/main/nextcode/emit-treecode-heapcleaner-calls-g.pkg

/*
Includes:
*/
#if NEED_HEAPCLEANER_PAUSE_STATISTICS		// Cleaner pause statistics are UNIX dependent.
    #include "system-dependent-unix-stuff.h"
#endif

#include "../mythryl-config.h"

#include <stdarg.h>
#include "runtime-base.h"
#include "runtime-configuration.h"
#include "get-multipage-ram-region-from-os.h"
#include "task.h"
#include "runtime-values.h"
#include "make-strings-and-vectors-etc.h"
#include "bigcounter.h"
#include "heap.h"
#include "runtime-globals.h"
#include "runtime-timer.h"
#include "heapcleaner-statistics.h"
#include "pthread-state.h"
#include "profiler-call-counts.h"

#ifdef C_CALLS
    // This is a list of pointers into the C heap locations that hold
    // pointers to Mythryl functions. This list is not part of any Mythryl data
    // package(s).  (also see src/c/heapcleaner/heapclean-n-agegroups.c and src/c/lib/ccalls/ccalls-fns.c)
    //
 extern Val	mythryl_functions_referenced_from_c_code_global;				// mythryl_functions_referenced_from_c_code_global	def in   src/c/lib/ccalls/ccalls-fns.c
#endif


void   call_heapcleaner   (Task* task,  int level) {
    // ================
    //
    // Clean the heap. We always clean agegroup0. If level is greater than
    // 0, or if agegroup0 full after cleaning, we also clean
    // one or more additional agegroups.  (A minimum of 'level' agegroups are cleaned.)

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS ];							// Registers and globals.
    Val** roots_ptr = roots;
    Heap* heap;


    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_MINOR_CLEANING );			// THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL is #defined      in	src/c/h/runtime-globals.h
												//  in terms of   this_fn_profiling_hook_refcell_global   from	src/c/main/construct-runtime-package.c

    #if NEED_PTHREAD_SUPPORT
	//
	// Signal all pthreads to enter heapcleaner mode and
	// select a designated pthread to do the heapcleaning work.
	// That pthread returns and falls into the regular heapcleaning code;
	// the remainder block at a barrier until heapcleaning is complete:
	//
	#ifdef NEED_PTHREAD_SUPPORT_DEBUG
	    debug_say ("igc %d\n", task->pid);
	#endif
	//
	int   active_pthreads_count;								// Number of active pthreads. We need this (only) for the final   pth_wait_at_barrier()   that 
	//											// releases the waiting pthreads in   pth_finish_heapcleaning from  src/c/heapcleaner/pthread-heapcleaner-stuff.c
	//
	if ((active_pthreads_count = pth_start_heapcleaning( task )) == 0) {			// pth_start_heapcleaning		def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
	    //
	    // Return value was zero, so we're not the designated heapcleaner pthread,
	    // and our return from pth_start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code.
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_RUNTIME );			// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1) We're the designated heapcleaner pthread.
	// 
	//     2) All other pthreads have now suspended execution of
	//        user code and are barrier-blocked waiting for us to
	//	  release them via the final pth_finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    #endif

    note_when_heapcleaning_started( task->heap );						// note_when_heapcleaning_started	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code_global;
    #endif

    #if NEED_PTHREAD_SUPPORT
	// Get extra roots from pthreads that entered
	// through call_heapcleaner_with_extra_roots
	//
	for (int i = 0;   pth_extra_heapcleaner_roots_global[i] != NULL;   i++) {
	    //
	    *roots_ptr++ =  pth_extra_heapcleaner_roots_global[i];
	}
    #endif

    // Gather ye roots while you may:
    //
    for (int i = 0;  i < c_roots_count_global;  i++)   {
	//
	*roots_ptr++ = c_roots_global[ i ];
    }

    #if !NEED_PTHREAD_SUPPORT
	//	
	*roots_ptr++ =  &task->link_register;
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
	//
    #else										// NEED_PTHREAD_SUPPORT
	{
	    Pthread*  pthread;
	    Task*     task;

	    for (int j = 0;  j < MAX_PTHREADS;  j++) {
		//
		pthread = pthread_table_global[ j ];

		task = pthread->task;

		#ifdef NEED_PTHREAD_SUPPORT_DEBUG
		    debug_say ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
		#endif

		if (pthread->status == PTHREAD_IS_RUNNING) {
		    //
		    *roots_ptr++ =  &task->argument;					// Why don't we here do &task->link_register, as above? ?  Why do we do is in the level > 0 case below?
		    *roots_ptr++ =  &task->fate;
		    *roots_ptr++ =  &task->closure;
		    *roots_ptr++ =  &task->exception_fate;
		    *roots_ptr++ =  &task->thread;
		    *roots_ptr++ =  &task->callee_saved_registers[0];
		    *roots_ptr++ =  &task->callee_saved_registers[1];
		    *roots_ptr++ =  &task->callee_saved_registers[2];
		}
	    }
	}
    #endif										// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );							// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 ilk is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt	size =   task->heap->agegroup0_buffer_bytesize;

	for (int i = 0;  i < MAX_PLAIN_ILKS;  i++) {
	    //
	    Sib* sib =  age1->sib[ i ];

	    if (sib_is_active( sib )							// sib_is_active		def in    src/c/h/heap.h
            &&  sib_freespace_in_bytes( sib ) < size					// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;								// Commit to multigeneration heapcleaning.
		break;
	    }
	}
    }

    if (level > 0) {
        //
	#if NEED_PTHREAD_SUPPORT
	    {
		Task* task;
		//
		for (int i = 0;  i < MAX_PTHREADS;  i++) {
		    //
		    Pthread*  pthread =  pthread_table_global[ i ];
		    //
		    task  =  pthread->task;
		    //
		    if (pthread->status == PTHREAD_IS_RUNNING) {
			//
			*roots_ptr++ =  &task->link_register;
		    }
		}
	    }
	#endif

	*roots_ptr = NULL;

	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_MAJOR_CLEANING );				// Remember that CPU cycles are charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );							// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset the generation0 allocation pointers:
    //
    #if NEED_PTHREAD_SUPPORT										// NB: Currently is this is TRUE then we require that NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS also be TRUE.
	//
	pth_finish_heapcleaning( task, active_pthreads_count );						// Multiple pthreads, so we must reset the generation-0 heap allocation pointers in each of them.
    #else
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if !NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT( heap );				// Set heap limit to obvious value.
	#else
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );			// Maybe set heap limit to artificially low value so as to regain control sooner to do software generated periodic event.
	#endif
    #endif

    note_when_cleaning_completed();									// note_when_cleaning_completed	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_RUNTIME );					// Remember that from here CPU cycles get charged to the runtime, not the heapcleaner.
}			                                             // fun call_heapcleaner


void   call_heapcleaner_with_extra_roots   (Task* task,  int level, ...)   {
    // =================================
    //
    // Clean with possible additional roots.  The list of
    // additional roots is NULL terminated.  We always clean agegroup0.
    // If level is greater than 0, or if agegroup 1 is full after cleaning
    // agegroup0, then we clean one or more additional agegroups.
    // At least 'level' agegroups are cleaned.
    //
    // NOTE: the multicore version of this may be BROKEN, since if a processor calls this
    // but isn't the collecting process, then THE EXTRA ROOTS ARE LOST.  XXX BUGGO FIXME

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS + MAX_EXTRA_HEAPCLEANER_ROOTS ];	// registers and globals
    Val** roots_ptr = roots;
    Val*  p;
    Heap* heap;

    va_list ap;

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_MINOR_CLEANING );					// Remember that CPU cycles after this get charged to the heapcleaner (generation0 pass).

    #if NEED_PTHREAD_SUPPORT
	//
	#ifdef NEED_PTHREAD_SUPPORT_DEBUG
	    debug_say ("igcwr %d\n", task->pid);
	#endif

	va_start (ap, level);

	int active_pthreads_count										// Number of active pthreads. We need this (only) for the final   pth_wait_at_barrier()   that 
	    =													// releases the waiting pthreads in   pth_finish_heapcleaning from  src/c/heapcleaner/pthread-heapcleaner-stuff.c
	    pth_call_heapcleaner_with_extra_roots (task, ap);							// pth_call_heapcleaner_with_extra_roots	def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c

	va_end(ap);

	if (active_pthreads_count == 0)	{
	    //
	    // Return value was zero, so we're not the designated heapcleaner pthread,
	    // and our return from pth_start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code.
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_RUNTIME );					// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1) We're the designated heapcleaner pthread.
	// 
	//     2) All other pthreads have now suspended execution of
	//        user code and are barrier-blocked waiting for us to
	//	  release them via the final pth_finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    #endif

    note_when_heapcleaning_started( task->heap );								// note_when_heapcleaning_started	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code_global;
    #endif

    #if NEED_PTHREAD_SUPPORT
        // Get extra roots from pthreads that entered through call_heapcleaner_with_extra_roots.
        // Our extra roots were placed in pth_extra_heapcleaner_roots_global by pth_call_heapcleaner_with_extra_roots.
        //
	for (int i = 0;  pth_extra_heapcleaner_roots_global[i] != NULL;  i++) {
	    //
	    *roots_ptr++ =  pth_extra_heapcleaner_roots_global[ i ];
	}
    #else
        // Note extra roots from argument list:
	//
	va_start (ap, level);
	//
	while ((p = va_arg(ap, Val *)) != NULL) {
	    //
	    *roots_ptr++ = p;
	}
	va_end(ap);
    #endif						// NEED_PTHREAD_SUPPORT

    // Gather the roots:
    //
    for (int i = 0;  i < c_roots_count_global;  i++) {
	//
	*roots_ptr++ =  c_roots_global[ i ];
    }

    #if !NEED_PTHREAD_SUPPORT
	//
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
	//
    #else						// NEED_PTHREAD_SUPPORT
	{
	    Task*     task;
	    Pthread*  pthread;

	    for (int j = 0;  j < MAX_PTHREADS;  j++) {
		//
		pthread = pthread_table_global[ j ];
		task    = pthread->task;

		#ifdef NEED_PTHREAD_SUPPORT_DEBUG
		    debug_say ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
		#endif

		if (pthread->status == PTHREAD_IS_RUNNING) {
		    //
		    *roots_ptr++ =  &task->argument;
		    *roots_ptr++ =  &task->fate;
		    *roots_ptr++ =  &task->closure;
		    *roots_ptr++ =  &task->exception_fate;
		    *roots_ptr++ =  &task->thread;
		    *roots_ptr++ =  &task->callee_saved_registers[ 0 ];
		    *roots_ptr++ =  &task->callee_saved_registers[ 1 ];
		    *roots_ptr++ =  &task->callee_saved_registers[ 2 ];
		}
	    }
	}
    #endif						// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );			// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 ilk is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt  size =  task->heap->agegroup0_buffer_bytesize;

	for (int i = 0;  i < MAX_PLAIN_ILKS;  i++) {
	    //
	    Sib* sib = age1->sib[ i ];
	    //
	    if (sib_is_active( sib )							// sib_is_active		def in    src/c/h/heap.h
            && (sib_freespace_in_bytes( sib ) < size)					// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;
		break;
	    }
	}
    }

    if (level > 0) {
	//
	#if !NEED_PTHREAD_SUPPORT
	    //
	    //
	    *roots_ptr++ =  &task->link_register;					// Why do we do this here but not in the above level > 0 case?
	    *roots_ptr++ =  &task->program_counter;
	#else
	    {   Pthread* pthread;
		//
		for (int i = 0;  i < MAX_PTHREADS;  i++) {
		    //
		    pthread = pthread_table_global[ i ];
		    //
		    if (pthread->status == PTHREAD_IS_RUNNING) {
			//
			*roots_ptr++ =  &pthread->task->link_register;
		    }
		}
	    }
	#endif

	*roots_ptr = NULL;

	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_MAJOR_CLEANING );				// Remember that CPU cycles are now being charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );								// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c

    }

    // Reset agegroup0 buffer:
    //
    #if NEED_PTHREAD_SUPPORT
        //
	pth_finish_heapcleaning (task, active_pthreads_count);
    #else
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );
	#else
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT(heap);
	#endif
    #endif

    note_when_cleaning_completed();										// note_when_cleaning_completed	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL_GLOBAL, PROF_RUNTIME );
}														// fun call_heapcleaner_with_extra_roots



Bool   need_to_call_heapcleaner   (Task* task,  Val_Sized_Unt nbytes)   {
    // ========================
    //
    // This fun is called various places to guarantee that there are 'nbytes'
    // free in the generation-zero buffer.  The canonical calls are those in
    //
    //     src/c/main/run-mythryl-code-and-runtime-eventloop.c
    //
    // This function is also (ab)used to trigger period-event processing by
    // either setting the end-of-heap limit artificially low, or else by
    // setting SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL_GLOBAL
    // to HEAP_TRUE.
    //
    // Check to see if a heapcleaning is required,
    // or if there is enough heap space
    // for nbytes worth of allocation.
    //	
    // Return TRUE, if the heapcleaner should be called,
    // FALSE otherwise.

    #if (NEED_PTHREAD_SUPPORT && NEED_PTHREAD_SUPPORT_FOR_SOFTWARE_GENERATED_PERIODIC_EVENTS)
	//
	if ((((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) task->heap_allocation_limit)
	|| (TAGGED_INT_TO_C_INT( SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL_GLOBAL) != 0))		// This appears to be set mainly (only?) in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
	//													// although it is also exported to the Mythryl level via   src/lib/std/src/unsafe/software-generated-periodic-events.api
    #elif NEED_PTHREAD_SUPPORT
	//
	if (((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) task->heap_allocation_limit)
	//
    #else
	if (((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) HEAP_ALLOCATION_LIMIT( task->heap ))
    #endif
	return TRUE;
    else
	return FALSE;
}


#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS

    void   reset_heap_allocation_limit_for_software_generated_periodic_events   (Task* task)   {
	// ==================================================================
	//
	// Reset the limit pointer according to the current polling frequency.

	int poll_frequency
	    = 
	    TAGGED_INT_TO_C_INT(DEREF(SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL_GLOBAL));	// SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL_GLOBAL is #defined in src/c/h/runtime-globals.h
													// in terms of software_generated_periodic_event_interval_refcell_global from src/c/main/construct-runtime-package.c
	Heap* heap =  task->heap;

	// Assumes heap_allocation_pointer has been reset:
	//
	task->real_heap_allocation_limit =  HEAP_ALLOCATION_LIMIT( heap );

	if (poll_frequency <= 0) {
	    //
	    task->heap_allocation_limit  = task->real_heap_allocation_limit;

	} else {

	    task->heap_allocation_limit  =  heap->agegroup0_buffer + poll_frequency * PERIODIC_EVENT_TIME_GRANULARITY_IN_NEXTCODE_INSTRUCTIONS;
	    //
	    task->heap_allocation_limit  =  (task->heap_allocation_limit > task->real_heap_allocation_limit)
		? task->real_heap_allocation_limit
		: task->heap_allocation_limit;
	}
    }
#endif						// NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS


// COPYRIGHT (c) 1993 by AT&T Bell Laboratories.
// Subsequent changes by Jeff Prothero Copyright (c) 2010-2011,
// released under Gnu Public Licence version 3.






/*
##########################################################################
#   The following is support for outline-minor-mode in emacs.		 #
#  ^C @ ^T hides all Text. (Leaves all headings.)			 #
#  ^C @ ^A shows All of file.						 #
#  ^C @ ^Q Quickfolds entire file. (Leaves only top-level headings.)	 #
#  ^C @ ^I shows Immediate children of node.				 #
#  ^C @ ^S Shows all of a node.						 #
#  ^C @ ^D hiDes all of a node.						 #
#  ^HFoutline-mode gives more details.					 #
#  (Or do ^HI and read emacs:outline mode.)				 #
#									 #
# Local variables:							 #
# mode: outline-minor							 #
# outline-regexp: "[A-Za-z]"			 		 	 #
# End:									 #
##########################################################################
*/
