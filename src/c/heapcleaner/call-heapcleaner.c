// call-heapcleaner.c
//
// The main interface between the heapcleaner
// ("garbage collector") and the rest of the
// run-time system.
//
// We may be manually invoked from the Mythryl level via
//
//     src/lib/std/src/nj/heapcleaner-control.pkg
//
// We may be automatically invoked via the CHECKLIMIT macro in various allocation fns in
//
//     src/c/machine-dependent/prim.intel32.asm
// 
// and thence via   system_run_mythryl_task_and_runtime_eventloop__may_heapclean/REQUEST_CLEANING   in
//
//     src/c/main/run-mythryl-code-and-runtime-eventloop.c
//
// to our   call_heapcleaner   entrypoint.
//
// More generally, we may be invoked via the heaplimit checks and heapcleaner calls generated by
//
//     src/lib/compiler/back/low/main/nextcode/pick-nextcode-fns-for-heaplimit-checks.pkg
//     src/lib/compiler/back/low/main/nextcode/emit-treecode-heapcleaner-calls-g.pkg

/*
Includes:
*/
#if NEED_HEAPCLEANER_PAUSE_STATISTICS		// Cleaner pause statistics are UNIX dependent.
    #include "system-dependent-unix-stuff.h"
#endif

#include "../mythryl-config.h"

#include <stdarg.h>
#include "runtime-base.h"
#include "runtime-configuration.h"
#include "get-quire-from-os.h"
#include "runtime-values.h"
#include "make-strings-and-vectors-etc.h"
#include "bigcounter.h"
#include "heap.h"
#include "runtime-globals.h"
#include "runtime-timer.h"
#include "heapcleaner-statistics.h"
#include "profiler-call-counts.h"

#ifdef C_CALLS													// C_CALLS is nowhere defined; it is referenced only in this file and in   src/c/lib/mythryl-callable-c-libraries-list.h
extern Val	mythryl_functions_referenced_from_c_code__global;						// mythryl_functions_referenced_from_c_code__global	def in   src/c/lib/ccalls/ccalls-fns.c
    //
    // This is a list of pointers into the C heap locations that hold
    // pointers to Mythryl functions. This list is not part of any Mythryl data
    // package(s).  (also see src/c/heapcleaner/heapclean-n-agegroups.c and src/c/lib/ccalls/ccalls-fns.c)
#endif


void   call_heapcleaner   (Task* task,  int level) {
    // ================
    //
    // Clean the heap. We always clean agegroup0. If level is greater than
    // 0, or if agegroup1 is full after cleaning, we also clean
    // one or more additional agegroups.  (A minimum of 'level' agegroups are cleaned.)

									    ENTER_MYTHRYL_CALLABLE_C_FN("call_heapcleaner");

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS ];									// Registers and globals.
    Val** roots_ptr = roots;
    Heap* heap;

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner/top" );					// check_agegroup0_overrun_tripwire_buffer	is from   src/c/heapcleaner/heap-debug-stuff.c

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MINOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that starting now CPU cycles are charged to the (minor) heapcleaner, not to the runtime or user code.
														// THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL is #defined      in	src/c/h/runtime-globals.h
														//  in terms of   this_fn_profiling_hook_refcell__global   from	src/c/main/construct-runtime-package.c

    #if NEED_PTHREAD_SUPPORT											// For background on the NEED_PTHREAD_SUPPORT stuff see the "Overview" comments in    src/lib/std/src/pthread.api
    {
	//
	// Signal all pthreads to enter heapcleaner mode and
	// select a PRIMARY_HEAPCLEANER pthread to do the heapcleaning work.
	// That pthread returns and falls into the regular heapcleaning code;
	// the remainder wait until heapcleaning is complete:
														PTHREAD_LOG_IF ("initiating heapcleaning mode tid d=%d\n", task->pthread->tid);
	//
	if (!pth__start_heapcleaning( task )) {									// pth__start_heapcleaning		def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
	    //
	    // Return value was FALSE, so we were a SECONDARY_HEAPCLEANER pthread,
	    // and our return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code:
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );			// Remember that starting now CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1) We are the PRIMARY_HEAPCLEANER pthread.
	// 
	//     2) No (other) pthreads are using the Mythryl heap, so it is safe
	//        to run the heapcleaner code, re-arranging the Mythryl heap.
	//
	// In more detail, the other threads are all
	// at this point in one of two modes:
	//
	//     A) pthread->mode == PTHREAD_IS_BLOCKED
	//
	//        These threads will not touch the Mythryl heap until we set
	//            pth__heapcleaner_state = HEAPCLEANER_IS_OFF
        //        thanks to the pthread_cond_wait() loop in
	//            recover_mythryl_heap()
	//        in src/c/pthread/pthread-on-posix-threads.c
	// 
	//     B) pthread->mode == PTHREAD_IS_SECONDARY_HEAPCLEANER
	//
	//	  These theads also will not touch the Mythryl heap until we set
	//            pth__heapcleaner_state = HEAPCLEANER_IS_OFF
	//        thanks here to the pthread_cond_wait() loop in
	//            pth__start_heapcleaning()
	//        in src/c/heapcleaner/pthread-heapcleaner-stuff.c
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }
    #endif

    note_when_heapcleaning_began( task->heap );									// note_when_heapcleaning_began	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    #if NEED_PTHREAD_SUPPORT
    {
        //
	// Get extra roots from pthreads that entered
	// through call_heapcleaner_with_extra_roots
	//
	for (int i = 0;   pth__extra_heapcleaner_roots__global[i] != NULL;   i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[i];
	}
    }
    #endif

    // Note a few C-level pointers into the Mythryl heap --
    // low-level special-case stuff like the signal handler,
    // runtime (pseudo-)package, pervasives etc:
    //
    for (int i = 0;  i < c_roots_count__global;  i++)   {							// c_roots_count__global		def in   src/c/main/construct-runtime-package.c
	//
	*roots_ptr++ =  c_roots__global[ i ];									// c_roots__global			def in   src/c/main/construct-runtime-package.c
    }

    // Note heapcleaner roots from the register set(s)
    // of the live Mythryl pthread task(s):
    //
    #if NEED_PTHREAD_SUPPORT
    {
	//
	for (int j = 0;  j < MAX_PTHREADS;  j++) {
	    //
	    Pthread* pthread =  pthread_table__global[ j ];
	    Task*    task    =  pthread->task;
														PTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (pthread->mode != PTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;								// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[0];
		*roots_ptr++ =  &task->callee_saved_registers[1];
		*roots_ptr++ =  &task->callee_saved_registers[2];
		*roots_ptr++ =   task->protected_c_arg;								// No '&' on this one -- it is a pointer to the value being protected.
	    }
	}
    }
    #endif													// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );										// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 sib is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // The critical consideration here is that during
    // heapclean_agegroup0(), as we copy stuff out of
    // agegroup0 we must not overflow any of the agegroup1
    // sibs (buffers).
    //    As a safe, conservative approximation, we
    // require that each agegroup1 sib have more free
    // space than the size of the complete agegroup0.
    //    (Obviously, this is vast overkill most of
    // the time, since typically agegroup0 is mostly
    // garbage, and the non-garbage will be typically
    // be spread over the various agegroup1 sibs.
    // But we have to code for the worst case.)
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt	agegroup0_bytesize =   agegroup0_buffer_size_in_bytes( task );

	for (int i = 0;  i < MAX_PLAIN_SIBS;  i++) {
	    //
	    Sib* sib =  age1->sib[ i ];

	    if (sib_is_active( sib )										// sib_is_active		def in    src/c/h/heap.h
            &&  sib_freespace_in_bytes( sib ) < agegroup0_bytesize						// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;											// Commit to multigeneration heapcleaning.
		break;
	    }
	}
    }

    if (level > 0) {
        //
	*roots_ptr = NULL;

	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MAJOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles are charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );								// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset the generation0 allocation pointers:
    //
    #if NEED_PTHREAD_SUPPORT											// NB: Currently is this is TRUE then we require that NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS also be TRUE.
    pth__finish_heapcleaning( task );										// Multiple pthreads, so we must reset the generation-0 heap allocation pointers in each of them.
    #else
    {
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if !NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	task->heap_allocation_limit
	    =
	    HEAP_ALLOCATION_LIMIT( task );
	#else
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );				// Maybe set heap limit to artificially low value so as to regain control sooner to do software generated periodic event.
	#endif
    }
    #endif

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner/bottom" );

    note_when_heapcleaning_ended();										// note_when_heapcleaning_ended	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );				// Remember that from here CPU cycles get charged to the runtime, not the heapcleaner.
}			 											// fun call_heapcleaner

void   call_heapcleaner_with_extra_roots   (Task* task,  int level,  Roots* extra_roots)   {
    // =================================
    //
    // Clean with possible additional roots.  The list of
    // additional roots is NULL terminated.  We always clean agegroup0.
    // If level is greater than 0, or if agegroup 1 is full after cleaning
    // agegroup0, then we clean one or more additional agegroups.
    // At least 'level' agegroups are cleaned.
    //
    // NOTE: the multicore version of this may be BROKEN, since if a processor calls this
    // but isn't the collecting process, then THE EXTRA ROOTS ARE LOST.  XXX BUGGO FIXME
														// MAX_EXTRA_HEAPCLEANER_ROOTS	def in   src/c/h/runtime-configuration.h
														ENTER_MYTHRYL_CALLABLE_C_FN("call_heapcleaner_with_extra_roots");

														// MAX_TOTAL_CLEANING_ROOTS	def in   src/c/h/runtime-configuration.h
    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS + MAX_EXTRA_HEAPCLEANER_ROOTS ];					// registers and globals
    Val** roots_ptr = roots;
    Heap* heap;

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner_with_extra_roots/top" );

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MINOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles after this get charged to the heapcleaner (generation0 pass).

    #if NEED_PTHREAD_SUPPORT
    {														PTHREAD_LOG_IF ("initiating heapcleaning mode (with roots) tid d=%d\n", task->pthread->tid);

	int we_are_the_primary_heapcleaner_pthread
	    =
	    pth__start_heapcleaning_with_extra_roots (task, extra_roots);					// pth__start_heapcleaning_with_extra_roots	def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c


	if (!we_are_the_primary_heapcleaner_pthread)	{
	    //
	    // We are not the primary heapcleaner pthread, and our
	    // return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code.
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );			// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1. We're the primary heapcleaner pthread.
	// 
	//     2. All other pthreads have now suspended execution of
	//        user code and are blocked waiting for us to
	//	  release them via the final pth__finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }
    #endif

    note_when_heapcleaning_began( task->heap );									// note_when_heapcleaning_began	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    #if NEED_PTHREAD_SUPPORT
    {
        // Get extra roots from pthreads that entered through call_heapcleaner_with_extra_roots.
        // Our extra roots were placed in pth__extra_heapcleaner_roots__global
        // by pth__start_heapcleaning_with_extra_roots.
        //
	for (int i = 0;  pth__extra_heapcleaner_roots__global[i] != NULL;  i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[ i ];
	}
    }
    #else
    {
        // Note extra_roots from argument list:
	//
	for (Roots* x = extra_roots;  x;  x = x->next) {
	    //
	    *roots_ptr++ =  x->root;
	}
    }
    #endif													// NEED_PTHREAD_SUPPORT

    // Note a few C-level pointers into the Mythryl heap --
    // low-level special-case stuff like the signal handler,
    // runtime (pseudo-)package, pervasives etc:
    //
    for (int i = 0;  i < c_roots_count__global;  i++) {
	//
	*roots_ptr++ =  c_roots__global[ i ];
    }

    // Note heapcleaner roots from the register set(s)
    // of the live Mythryl pthread task(s):
    //
    #if NEED_PTHREAD_SUPPORT
    {
	//
	Task*     task;
	Pthread*  pthread;

	for (int j = 0;  j < MAX_PTHREADS;  j++) {
	    //
	    pthread = pthread_table__global[ j ];

	    task    = pthread->task;
														PTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (pthread->mode != PTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;								// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[ 0 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 1 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 2 ];
		*roots_ptr++ =   task->protected_c_arg;								// No '&' on this one -- it is a pointer to the value being protected.
	    }
	}
    }
    #else
	//
	*roots_ptr++ =  &task->link_register;
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->current_closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->current_thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
	*roots_ptr++ =   task->protected_c_arg;									// No '&' on this one -- it is a pointer to the value being protected.
    #endif													// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );										// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 ilk is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt	agegroup0_bytesize =   agegroup0_buffer_size_in_bytes( task );

	for (int i = 0;  i < MAX_PLAIN_SIBS;  i++) {
	    //
	    Sib* sib = age1->sib[ i ];
	    //
	    if (sib_is_active( sib )										// sib_is_active		def in    src/c/h/heap.h
            && (sib_freespace_in_bytes( sib ) < agegroup0_bytesize)						// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;
		break;
	    }
	}
    }

    if (level > 0) {
	//
	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MAJOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles are now being charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );								// heapclean_n_agegroups	def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset agegroup0 buffer:
    //
    #if NEED_PTHREAD_SUPPORT
    pth__finish_heapcleaning( task );
    #else
	task->heap_allocation_pointer = task->heap_allocation_buffer;

	#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );
	#else
	    task->heap_allocation_limit
		=
	        HEAP_ALLOCATION_LIMIT( task );
	#endif
    #endif

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner_with_extra_roots/bottom" );

    note_when_heapcleaning_ended();										// note_when_heapcleaning_ended	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );				// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
}														// fun call_heapcleaner_with_extra_roots



Bool   need_to_call_heapcleaner   (Task* task,  Val_Sized_Unt bytes_needed)   {
    // ========================
    //
    // This fun is called various places to guarantee that there are 'bytes_needed'
    // free in the generation-zero buffer.  The canonical calls are those in
    //
    //     src/c/main/run-mythryl-code-and-runtime-eventloop.c
    //
    // This function is also (ab)used to trigger period-event processing by
    // either setting the end-of-heap limit artificially low, or else by
    // setting SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL
    // to HEAP_TRUE.
    //
    // Check to see if a heapcleaning is required,
    // or if there is enough heap space
    // for bytes_needed worth of allocation.
    //	
    // Return TRUE, if the heapcleaner should be called,
    // FALSE otherwise.

// There was a #if NEED_PTHREAD_SUPPORT here but the logic was so complex I dropped it to simplify things... 2011-11-12 CrT
    {
        if (pth__heapcleaner_state != HEAPCLEANER_IS_OFF)   return TRUE;

    #if NEED_PTHREAD_SUPPORT_FOR_SOFTWARE_GENERATED_PERIODIC_EVENTS
	//
	return (((Punt)(task->heap_allocation_pointer)+bytes_needed) >= (Punt) HEAP_ALLOCATION_LIMIT( task ))
	    || (DEREF( SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL) == HEAP_TRUE);		// This appears to be set mainly (only?) in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
														// although it is also exported to the Mythryl level via   src/lib/std/src/unsafe/software-generated-periodic-events.api
    #else
	//
	return   ((Punt)(task->heap_allocation_pointer)+bytes_needed) >= (Punt) HEAP_ALLOCATION_LIMIT(task);	// HEAP_ALLOCATION_LIMIT	is #defined in   src/c/h/heap.h
    #endif
    }
//    #endif
}


#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS

    void   reset_heap_allocation_limit_for_software_generated_periodic_events   (Task* task)   {
	// ==================================================================
	//
	// Reset the limit pointer according to the current polling frequency.

	int poll_frequency
	    = 
	    TAGGED_INT_TO_C_INT(DEREF(SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL));	// SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL is #defined in src/c/h/runtime-globals.h
													// in terms of software_generated_periodic_event_interval_refcell__global from src/c/main/construct-runtime-package.c
	task->real_heap_allocation_limit =  HEAP_ALLOCATION_LIMIT( task );

	//
        zero_agegroup0_overrun_tripwire_buffer( task );
	

	if (poll_frequency <= 0) {
	    //
	    task->heap_allocation_limit  = task->real_heap_allocation_limit;

	} else {

	    task->heap_allocation_limit  =  task->heap_allocation_buffer + poll_frequency * PERIODIC_EVENT_TIME_GRANULARITY_IN_NEXTCODE_INSTRUCTIONS;
	    //
	    task->heap_allocation_limit  =  MIN( task->real_heap_allocation_limit, task->heap_allocation_limit );
	}
    }
#endif						// NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS


// COPYRIGHT (c) 1993 by AT&T Bell Laboratories.
// Subsequent changes by Jeff Prothero Copyright (c) 2010-2011,
// released under Gnu Public Licence version 3.






/*
##########################################################################
#   The following is support for outline-minor-mode in emacs.		 #
#  ^C @ ^T hides all Text. (Leaves all headings.)			 #
#  ^C @ ^A shows All of file.						 #
#  ^C @ ^Q Quickfolds entire file. (Leaves only top-level headings.)	 #
#  ^C @ ^I shows Immediate children of node.				 #
#  ^C @ ^S Shows all of a node.						 #
#  ^C @ ^D hiDes all of a node.						 #
#  ^HFoutline-mode gives more details.					 #
#  (Or do ^HI and read emacs:outline mode.)				 #
#									 #
# Local variables:							 #
# mode: outline-minor							 #
# outline-regexp: "[A-Za-z]"			 		 	 #
# End:									 #
##########################################################################
*/
