// call-heapcleaner.c
//
// The main interface between the heapcleaner
// ("garbage collector") and the rest of the
// run-time system.
//
// We may be manually invoked from the Mythryl level via
//
//     src/lib/std/src/nj/heapcleaner-control.pkg
//
// We may be automatically invoked via the CHECKLIMIT macro in various allocation fns in
//
//     src/c/machine-dependent/prim.intel32.asm
// 
// and thence via   system_run_mythryl_task_and_runtime_eventloop__may_heapclean/REQUEST_HEAPCLEANING   in
//
//     src/c/main/run-mythryl-code-and-runtime-eventloop.c
//
// to our   call_heapcleaner   entrypoint.
//
// More generally, we may be invoked via the heaplimit checks and heapcleaner calls generated by
//
//     src/lib/compiler/back/low/main/nextcode/pick-nextcode-fns-for-heaplimit-checks.pkg
//     src/lib/compiler/back/low/main/nextcode/emit-treecode-heapcleaner-calls-g.pkg

/*
Includes:
*/
#if NEED_HEAPCLEANER_PAUSE_STATISTICS		// Cleaner pause statistics are UNIX dependent.
    #include "system-dependent-unix-stuff.h"
#endif

#include "../mythryl-config.h"

#include <stdarg.h>
#include "runtime-base.h"
#include "runtime-configuration.h"
#include "get-quire-from-os.h"
#include "runtime-values.h"
#include "make-strings-and-vectors-etc.h"
#include "bigcounter.h"
#include "heap.h"
#include "runtime-globals.h"
#include "runtime-timer.h"
#include "heapcleaner-statistics.h"
#include "profiler-call-counts.h"

#ifdef C_CALLS													// C_CALLS is nowhere defined; it is referenced only in this file and in   src/c/lib/mythryl-callable-c-libraries-list.h
extern Val	mythryl_functions_referenced_from_c_code__global;						// mythryl_functions_referenced_from_c_code__global	def in   src/c/lib/ccalls/ccalls-fns.c
    //
    // This is a list of pointers into the C heap locations that hold
    // pointers to Mythryl functions. This list is not part of any Mythryl data
    // package(s).  (also see src/c/heapcleaner/heapclean-n-agegroups.c and src/c/lib/ccalls/ccalls-fns.c)
#endif


void   call_heapcleaner   (Task* task,  int level) {
    // ================
    //
    // Clean the heap.
    // We always clean agegroup0.
    // If 'level' is greater than 0, or if agegroup1
    // is full after cleaning, we also clean one or more
    // additional agegroups.  (A minimum of 'level' agegroups are cleaned.)

static int callcount = 0;
if (! (++callcount & 0xff)) log_if ("call_heapcleaner:  task %p  level %d  callcount %d",    task, level, callcount);
														ENTER_MYTHRYL_CALLABLE_C_FN(__func__);

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS ];			// Registers and globals.			// MAX_TOTAL_CLEANING_ROOTS				is from   src/c/h/runtime-configuration.h
    Val** roots_ptr = roots;
    Heap* heap;


    // This is support for Mythryl-level code being able
    // to register a 'signal handler' for the pseudo-signal
    // of a heapcleaning being done:
    //
// Currently uncommenting this here and balow yields
// bin/mythryld: Fatal error:  Uncaught exception FAIL with "inconsistent state IGNORE for signal 30" raised at src/lib/std/src/nj/interprocess-signals-guts.pkg:620.25-620.120
//    task->hostthread->posix_signal_counts[ RUNSIG_HEAPCLEANING_DONE ].seen_count++;
//    task->hostthread->all_posix_signals.seen_count++;


    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner/top" );					// check_agegroup0_overrun_tripwire_buffer		is from   src/c/heapcleaner/heap-debug-stuff.c

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MINOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that starting now CPU cycles are charged to the (minor) heapcleaner, not to the runtime or user code.
														// THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL is #defined	in   src/c/h/runtime-globals.h
														//  in terms of   this_fn_profiling_hook_refcell__global   from	src/c/main/construct-runtime-package.c

    {														// Hostthread support -- for background see the "Overview" comments in    src/lib/std/src/hostthread.api
	//
	// Signal all hostthreads to enter heapcleaner mode and
	// select a PRIMARY_HEAPCLEANER hostthread to do the heapcleaning work.
	// That hostthread returns and falls into the regular heapcleaning code;
	// the remainder wait until heapcleaning is complete:
														HOSTTHREAD_LOG_IF ("initiating heapcleaning mode tid x=%lx\n", (unsigned long int) task->hostthread->ptid);
	//
	if (!pth__start_heapcleaning( task )) {									// pth__start_heapcleaning				def in   src/c/heapcleaner/hostthread-heapcleaner-stuff.c
	    //
	    // Return value was FALSE, so we were a SECONDARY_HEAPCLEANER hostthread,
	    // and our return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code:
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );			// Remember that starting now CPU cycles are charged to the runtime, not the heapcleaner.
	    //
														EXIT_MYTHRYL_CALLABLE_C_FN(__func__);
	    return;
	}
ramlog_printf("#%d call_heapcleaner/AAA\n", syscalls_seen );

	// At this point we know that
	// 
	//     1) We are the PRIMARY_HEAPCLEANER hostthread.
	// 
	//     2) No (other) hostthreads are using the Mythryl heap, so it is safe
	//        to run the heapcleaner code, re-arranging the Mythryl heap.
	//
	// In more detail, the other threads are all
	// at this point in one of two modes:
	//
	//     A) hostthread->mode == HOSTTHREAD_IS_BLOCKED
	//
	//        These threads will not touch the Mythryl heap until we set
	//            pth__heapcleaner_state__global = HEAPCLEANER_IS_OFF
        //        thanks to the pthread_cond_wait() loop in
	//            recover_mythryl_heap()
	//        in src/c/hostthread/hostthread-on-posix-threads.c
	// 
	//     B) hostthread->mode == HOSTTHREAD_IS_SECONDARY_HEAPCLEANER
	//
	//	  These theads also will not touch the Mythryl heap until we set
	//            pth__heapcleaner_state__global = HEAPCLEANER_IS_OFF
	//        thanks here to the pthread_cond_wait() loop in
	//            pth__start_heapcleaning()
	//        in src/c/heapcleaner/hostthread-heapcleaner-stuff.c
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }

    note_when_heapcleaning_began( task->heap );									// note_when_heapcleaning_began	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    {   // Get extra roots from hostthreads that entered
	// through call_heapcleaner_with_extra_roots
	//
	for (int i = 0;   pth__extra_heapcleaner_roots__global[i] != NULL;   i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[i];
	}
    }

    // Note a few C-level pointers into the Mythryl heap --
    // low-level special-case stuff like the signal handler,
    // runtime (pseudo-)package, pervasives etc:
    //
    for (int i = 0;  i < c_roots_count__global;  i++)   {							// c_roots_count__global		def in   src/c/main/construct-runtime-package.c
	//
	*roots_ptr++ =  c_roots__global[ i ];									// c_roots__global			def in   src/c/main/construct-runtime-package.c
    }

    // Note heapcleaner roots from the register set(s)
    // of the live Mythryl hostthread task(s):
    //
    {   for (int j = 0;  j < MAX_HOSTTHREADS;  j++) {
	    //
	    Hostthread* hostthread =  hostthread_table__global[ j ];
	    Task*    task    =  hostthread->task;
														HOSTTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (hostthread->mode != HOSTTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;								// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[0];
		*roots_ptr++ =  &task->callee_saved_registers[1];
		*roots_ptr++ =  &task->callee_saved_registers[2];
		*roots_ptr++ =   task->protected_c_arg;								// No '&' on this one -- it is a pointer to the value being protected.
	    }
	}
    }

    *roots_ptr = NULL;
														if (roots_ptr >= &roots[ MAX_TOTAL_CLEANING_ROOTS ]) {
														    die( "src/c/heapcleaner/call-heapcleaner.c: call_heapcleaner: roots[] overflow, increase MAX_TOTAL_CLEANING_ROOTS" );
														}

    heapclean_agegroup0( task, roots );										// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any agegroup-1 sib is short on freespace,
    // commit to doing a multi-agegroup heapcleaning.
    //
    // The critical consideration here is that during
    // heapclean_agegroup0(), as we copy stuff out of
    // agegroup0 we must not overflow any of the agegroup1
    // sibs (buffers).
    //    As a safe, conservative approximation, we
    // require that each agegroup1 sib have more free
    // space than the size of the complete agegroup0.
    //    (Obviously, this is vast overkill most of
    // the time, since typically agegroup0 is mostly
    // garbage, and the non-garbage will be typically
    // be spread over the various agegroup1 sibs.
    // But we have to code for the worst case.)
    //
    // We can skip this check if we're anyhow already
    // committed to    a multi-agegroup heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Vunt	agegroup0_bytesize =   agegroup0_buffer_size_in_bytes( task );

	for (int i = 0;  i < MAX_PLAIN_SIBS;  i++) {
	    //
	    Sib* sib =  age1->sib[ i ];

	    if (sib_is_active( sib )										// sib_is_active		def in    src/c/h/heap.h
            &&  sib_freespace_in_bytes( sib ) < agegroup0_bytesize						// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;											// Commit to multi-agegroup heapcleaning.
		break;
	    }
	}
    }

    if (level > 0) {
        //
	*roots_ptr = NULL;

	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MAJOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles are charged to the heapcleaner (multi-agegroup pass).

	heapclean_n_agegroups( task, roots, level );								// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset the agegroup0 allocation pointers:
    //														// NB: Currently this code requires that NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS be TRUE.
    pth__finish_heapcleaning( task );										// Multiple hostthreads, so we must reset the agegroup-0 heap allocation pointers in each of them.

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner/bottom" );

    note_when_heapcleaning_ended();										// note_when_heapcleaning_ended	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );				// Remember that from here CPU cycles get charged to the runtime, not the heapcleaner.

														EXIT_MYTHRYL_CALLABLE_C_FN(__func__);
}			 											// fun call_heapcleaner



void   call_heapcleaner_with_extra_roots   (Task* task,  int level,  Roots* extra_roots)   {
    // =================================
    //
    // Clean with possible extra roots.
    //
    // The extra_roots list is NULL terminated.
    //
    // We always clean agegroup0.
    //
    // If 'level' is greater than 0, or if agegroup 1 is full after cleaning
    // agegroup0, then we clean one or more additional agegroups.
    // At least 'level' agegroups are cleaned.
    //
    // NOTE: the multicore version of this may be BROKEN, since if a hostthread calls this
    // but isn't the collecting hostthread, then THE EXTRA ROOTS ARE LOST.  XXX BUGGO FIXME
    // 2012-02-28 CrT: Previous two lines are 20 years old;
    //                 I believe they are no longer true.
														ENTER_MYTHRYL_CALLABLE_C_FN(__func__);

														// MAX_EXTRA_HEAPCLEANER_ROOTS_PER_HOSTTHREAD	def in   src/c/h/runtime-configuration.h
														// MAX_TOTAL_CLEANING_ROOTS			def in   src/c/h/runtime-configuration.h
    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS + MAX_EXTRA_HEAPCLEANER_ROOTS_PER_HOSTTHREAD ];				// registers and globals
    Val** roots_ptr = roots;

    Heap* heap;


    // This is support for Mythryl-level code being able
    // to register a 'signal handler' for the pseudo-signal
    // of a heapcleaning being done:
    //
// Currently uncommenting this here and above yields
// bin/mythryld: Fatal error:  Uncaught exception FAIL with "inconsistent state IGNORE for signal 30" raised at src/lib/std/src/nj/interprocess-signals-guts.pkg:620.25-620.120
//    task->hostthread->posix_signal_counts[ RUNSIG_HEAPCLEANING_DONE ].seen_count++;
//    task->hostthread->all_posix_signals.seen_count++;


    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner_with_extra_roots/top" );

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MINOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles after this get charged to the heapcleaner (agegroup-0 pass).

    {														// Hostthread support.
														HOSTTHREAD_LOG_IF ("initiating heapcleaning mode (with roots) tid x=%lx\n", (unsigned long int) task->hostthread->ptid);
	int we_are_the_primary_heapcleaner_hostthread
	    =
	    pth__start_heapcleaning_with_extra_roots (task, extra_roots);					// pth__start_heapcleaning_with_extra_roots	def in   src/c/heapcleaner/hostthread-heapcleaner-stuff.c


	if (!we_are_the_primary_heapcleaner_hostthread)	{
	    //
	    // We are not the primary heapcleaner hostthread, and our
	    // return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code.
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );			// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
	    //
														EXIT_MYTHRYL_CALLABLE_C_FN(__func__);
	    return;
	}

	// At this point we know that
	// 
	//     1. We're the primary heapcleaner hostthread.
	// 
	//     2. All other hostthreads have now suspended execution of
	//        user code and are blocked waiting for us to
	//	  release them via the final pth__finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }

    note_when_heapcleaning_began( task->heap );									// note_when_heapcleaning_began	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    {														// Hostthread support.
        // Get extra roots from hostthreads that entered through call_heapcleaner_with_extra_roots.
        // Our extra roots were placed in pth__extra_heapcleaner_roots__global
        // by pth__start_heapcleaning_with_extra_roots.
        //
	for (int i = 0;  pth__extra_heapcleaner_roots__global[i] != NULL;  i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[ i ];
	}
    }

    // Note a few C-level pointers into the Mythryl heap --
    // low-level special-case stuff like the signal handler,
    // runtime (pseudo-)package, pervasives etc:
    //
    for (int i = 0;  i < c_roots_count__global;  i++) {
	//
	*roots_ptr++ =  c_roots__global[ i ];
    }

    // Note heapcleaner roots from the register set(s)
    // of the live Mythryl hostthread task(s):
    //
    {   Task*     task;
	Hostthread*  hostthread;

	for (int j = 0;  j < MAX_HOSTTHREADS;  j++) {
	    //
	    hostthread = hostthread_table__global[ j ];

	    task    = hostthread->task;
														HOSTTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (hostthread->mode != HOSTTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;								// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[ 0 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 1 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 2 ];
		*roots_ptr++ =   task->protected_c_arg;								// No '&' on this one -- it is a pointer to the value being protected.
	    }
	}
    }

    *roots_ptr = NULL;

														if (roots_ptr >=  &roots[ MAX_TOTAL_CLEANING_ROOTS ]) {
														    die("src/c/heapcleaner/call-heapcleaner.c: roots[] overflow.");
														} 

    heapclean_agegroup0( task, roots );										// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any agegroup-1 sib buffer is short on freespace,
    // commit to doing a multi-agegroup heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multi-agegroup heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Vunt	agegroup0_bytesize =   agegroup0_buffer_size_in_bytes( task );

	for (int i = 0;  i < MAX_PLAIN_SIBS;  i++) {
	    //
	    Sib* sib = age1->sib[ i ];
	    //
	    if (sib_is_active( sib )										// sib_is_active		def in    src/c/h/heap.h
            && (sib_freespace_in_bytes( sib ) < agegroup0_bytesize)						// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;
		break;
	    }
	}
    }

    if (level > 0) {
	//
	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_MAJOR_HEAPCLEANER__CPU_USER_INDEX );			// Remember that CPU cycles are now being charged to the heapcleaner (multi-agegroup pass).

	heapclean_n_agegroups( task, roots, level );								// heapclean_n_agegroups	def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset agegroup0 buffer:
    //
    pth__finish_heapcleaning( task );										// Hostthread support.

    check_agegroup0_overrun_tripwire_buffer( task, "call_heapcleaner_with_extra_roots/bottom" );

    note_when_heapcleaning_ended();										// note_when_heapcleaning_ended	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, IN_RUNTIME__CPU_USER_INDEX );				// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.

														EXIT_MYTHRYL_CALLABLE_C_FN(__func__);

}														// fun call_heapcleaner_with_extra_roots



Bool   need_to_call_heapcleaner   (Task* task,  Vunt bytes_needed)   {
    // ========================
    //
    // This fun is called various places to guarantee that there are 'bytes_needed'
    // free in the agegroup-zero buffer.  The canonical calls are those in
    //
    //     src/c/main/run-mythryl-code-and-runtime-eventloop.c
    //
    // This function is also (ab)used to trigger period-event processing by
    // either setting the end-of-heap limit artificially low, or else by
    // setting SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL
    // to HEAP_TRUE.
    //
    // Check to see if a heapcleaning is required,
    // or if there is enough heap space
    // for bytes_needed worth of allocation.
    //	
    // Return TRUE, if the heapcleaner should be called,
    // FALSE otherwise.

    {														// Hostthread support.
        if (pth__heapcleaner_state__global != HEAPCLEANER_IS_OFF)   return TRUE;

    #if NEED_HOSTTHREAD_SUPPORT_FOR_SOFTWARE_GENERATED_PERIODIC_EVENTS
	//
	return (((Vunt)(task->heap_allocation_pointer)+bytes_needed) >= (Vunt) HEAP_ALLOCATION_LIMIT( task ))	// HEAP_ALLOCATION_LIMIT	is from   src/c/h/heap.h
	    || (DEREF( SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL) == HEAP_TRUE);		// This appears to be set mainly (only?) in   src/c/heapcleaner/hostthread-heapcleaner-stuff.c
														// although it is also exported to the Mythryl level via   src/lib/std/src/unsafe/software-generated-periodic-events.api
    #else
	//
	return   ((Vunt)(task->heap_allocation_pointer)+bytes_needed) >= (Vunt) HEAP_ALLOCATION_LIMIT(task);	// HEAP_ALLOCATION_LIMIT	is #defined in   src/c/h/heap.h
    #endif
    }
}


#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS

    void   reset_heap_allocation_limit_for_software_generated_periodic_events   (Task* task)   {
	// ==================================================================
	//
	// Reset the limit pointer according to the current polling frequency.

	int poll_frequency
	    = 
	    TAGGED_INT_TO_C_INT(DEREF(SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL));	// SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL is #defined in src/c/h/runtime-globals.h
													// in terms of software_generated_periodic_event_interval_refcell__global from src/c/main/construct-runtime-package.c
	task->real_heap_allocation_limit =  HEAP_ALLOCATION_LIMIT( task );

	//
        initialize_agegroup0_overrun_tripwire_buffer( task );
	

	if (poll_frequency <= 0) {
	    //
	    task->heap_allocation_limit  = task->real_heap_allocation_limit;

	} else {

	    task->heap_allocation_limit  =  task->heap_allocation_buffer + poll_frequency * PERIODIC_EVENT_TIME_GRANULARITY_IN_NEXTCODE_INSTRUCTIONS;
	    //
	    task->heap_allocation_limit  =  MIN( task->real_heap_allocation_limit, task->heap_allocation_limit );
	}
    }
#endif						// NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS


// COPYRIGHT (c) 1993 by AT&T Bell Laboratories.
// Subsequent changes by Jeff Prothero Copyright (c) 2010-2012,
// released per terms of SMLNJ-COPYRIGHT.






/*
##########################################################################
#   The following is support for outline-minor-mode in emacs.		 #
#  ^C @ ^T hides all Text. (Leaves all headings.)			 #
#  ^C @ ^A shows All of file.						 #
#  ^C @ ^Q Quickfolds entire file. (Leaves only top-level headings.)	 #
#  ^C @ ^I shows Immediate children of node.				 #
#  ^C @ ^S Shows all of a node.						 #
#  ^C @ ^D hiDes all of a node.						 #
#  ^HFoutline-mode gives more details.					 #
#  (Or do ^HI and read emacs:outline mode.)				 #
#									 #
# Local variables:							 #
# mode: outline-minor							 #
# outline-regexp: "[A-Za-z]"			 		 	 #
# End:									 #
##########################################################################
*/
